{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NLP 2025\n",
    "# Lab 1: Tokenization\n",
    "\n",
    "Tokenization is a fundamental step in **Natural Language Processing (NLP)** 🧠💬 that transforms raw text into structured data for computational models. In this lab, you will explore different **tokenization techniques** 📝, preprocess text data 🔍, and implement **tokenization pipelines** using popular NLP libraries 🏗️.  \n",
    "\n",
    "You will also gain **hands-on experience** with **Hugging Face Datasets 🤗📚**, while assessing the impact of tokenization choices on downstream NLP tasks. \n",
    "\n",
    "By the end of this lab, you will have a **strong foundation** in tokenization techniques and be able to apply them effectively in **real-world NLP applications** 🌍.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Learning Goals**  \n",
    "\n",
    "By the end of this lab, you should be able to:  \n",
    "\n",
    "✅ **Understand the role of tokenization in NLP** 🧠💡  \n",
    "✅ **Explain why tokenization is important** and how it affects text processing 📖🔍  \n",
    "✅ **Implement different tokenization techniques** – Apply **word** 📝, **subword** 🔢, and **character-level** 🔠 tokenization using built-in libraries.  \n",
    "✅ **Use Hugging Face Datasets** 🤗📊 – Load and preprocess text datasets efficiently.  \n",
    "✅ **Evaluate tokenization impact** 📉🔎 – Analyze how different tokenization methods influence model performance.  \n",
    "✅ **Identify challenges in tokenization** ❗🔍 – Recognize issues like **out-of-vocabulary (OOV) words**, **ambiguity**, and **multilingual tokenization** 🌍.  \n",
    "\n",
    "### Score breakdown\n",
    "\n",
    "| Exercise            | Points |\n",
    "|---------------------|--------|\n",
    "| [Exercise 1](#e1)   | 5      |\n",
    "| [Exercise 2](#e2)   | 6      |\n",
    "| [Exercise 3](#e3)   | 5      |\n",
    "| [Exercise 4](#e4)   | 12     |\n",
    "| [Exercise 5](#e5)   | 5      |\n",
    "| [Exercise 6](#e6)   | 22     |\n",
    "| [Exercise 7](#e7)   | 5      |\n",
    "| [Exercise 8](#e8)   | 5      |\n",
    "| [Exercise 9](#e9)   | 10     |\n",
    "| [Exercise 10](#e10) | 25     |\n",
    "| Total               | 100    |\n",
    "\n",
    "This score will be scaled down to 0.5 and that will be your final lab score.\n",
    "\n",
    "### 📌 **Instructions for Delivery** (📅 **Deadline: 11/Apr 18:00**, 🎭 *wildcards possible*)  \n",
    "\n",
    "✅ **Submission Requirements**  \n",
    "+ 📄 You need to submit a **PDF of your report** (use the templates provided in **LaTeX** 🖋️ (*preferred*) or **Word** 📑) and a **copy of your notebook** 📓 with the code.  \n",
    "+ ⚡ Make sure that **all cells are executed properly** ⚙️ and that **all figures/results/plots** 📊 you include in the report are also visible in your **executed notebook**.  \n",
    "\n",
    "✅ **Collaboration & Integrity**  \n",
    "+ 🗣️ While you may **discuss** the lab with others, you must **write your solutions with your group only**. If you **discuss specific tasks** with others, please **include their names** in the appendix of the report.  \n",
    "+ 📜 **Honor Code applies** to this lab. For more details, check **Syllabus §7.2** ⚖️.  \n",
    "+ 📢 **Mandatory Disclosure**:  \n",
    "   - Any **websites** 🌐 (e.g., **Stack Overflow** 💡) or **other resources** used must be **listed and disclosed**.  \n",
    "   - Any **GenAI tools** 🤖 (e.g., **ChatGPT**) used must be **explicitly mentioned**.  \n",
    "   - 🚨 **Failure to disclose these resources is a violation of academic integrity**. See **Syllabus §7.3** for details.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:52.810733Z",
     "start_time": "2025-04-11T09:39:52.796473Z"
    }
   },
   "source": [
    "# ! pip install -U datasets~=3.2.0\n",
    "# ! python -m pip install -U matplotlib"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:53.962184Z",
     "start_time": "2025-04-11T09:39:52.831526Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from debugpy.common.sockets import shut_down"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariowettig/PycharmProjects/NLP_Labs/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Intro to regular expressions\n",
    "\n",
    "In this introduction section, you can practice the use of regular expressions in python. You can find the documentation here: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html). The main functions of the re module are:\n",
    "- `re.search()` - searches for a pattern in a string, returns the first match,\n",
    "- `re.findall()` - similar to `search()`, but returns a list of all matches,\n",
    "- `re.sub()` - replaces the matches with a string.\n",
    "\n",
    "All above functions accept the regular expression pattern as their argument. The patterns are strings that represent the rules for matching the text. In python they start with `r` character, e.g. `r'\\d'` is a pattern that matches a digit.\n",
    "\n",
    "Let us start with a simple example. We will search for the word \"world\" in the string \"Hello, world!\"."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.161763Z",
     "start_time": "2025-04-11T09:39:54.157033Z"
    }
   },
   "source": [
    "text = \"Hello, world!\"\n",
    "pattern = r'world'\n",
    "match = re.search(pattern, text)\n",
    "print(match)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 12), match='world'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `search()` function returns a match object that tells us where the match was found (`span` argument) and the exact part of the string that matched the pattern (`group` argument).\n",
    "\n",
    "Below you can find the examples from the lecture."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.479818Z",
     "start_time": "2025-04-11T09:39:54.473936Z"
    }
   },
   "source": [
    "# Disjunctions\n",
    "pattern = r'[wW]oodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "pattern = r'[1234567890]' # matches any digit\n",
    "pattern = r'[0-9]' # matches any digit\n",
    "pattern = r'[A-Z]' # matches any uppercase letter\n",
    "pattern = r'[a-z]' # matches any lowercase letter\n",
    "pattern = r'[A-Za-z]' # matches any letter\n",
    "\n",
    "# Disjunctions with pipe |\n",
    "pattern = r'groundhog|Woodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "\n",
    "# Negation (only when in [])\n",
    "pattern = r'[^0-9]' # matches any character that is not a digit\n",
    "pattern = r'[^Ss]' # matches any character that is not 'S' or 's'\n",
    "pattern = r'a^b' # matches the string \"a^b\"\n",
    "\n",
    "# Quantifiers (+, *, ?, .)\n",
    "pattern = r'baa+' # matches \"ba\" followed by one or more \"a\" (e.g. \"baa\", \"baaa\", \"baaaa\", ...)\n",
    "pattern = r'oo*h' # matches \"o\" followed by zero or more \"o\" and then \"h\" (e.g. \"oh\", \"ooh\", \"oooh\", ...)\n",
    "pattern = r'colou?r' # matches \"color\" and \"colour\"\n",
    "pattern = r'beg.n' # matches \"begun\", \"begin\", \"begnn\", ...\n",
    "\n",
    "# Anchors (^, $)\n",
    "pattern = r'^Hello' # matches \"Hello\" at the beginning of the string\n",
    "pattern = r'world!$' # matches \"world!\" at the end of the string"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Huggingface datasets\n",
    "\n",
    "For this lab, we will use the **Hugging Face Datasets** library ([Hugging Face Datasets](https://huggingface.co/datasets)), which provides an extensive collection of ready-to-use NLP datasets. The library is designed to be lightweight, efficient, and compatible with popular deep learning frameworks such as PyTorch and TensorFlow.  \n",
    "\n",
    "You can find the full documentation and tutorials here:  \n",
    "📌 [Hugging Face Datasets Documentation](https://huggingface.co/docs/datasets/en/index)  \n",
    "\n",
    "### **Why use Hugging Face Datasets?**  \n",
    "- **Easy Access:** Load datasets with a single command without manual downloads.  \n",
    "- **Standardized Format:** Datasets come in a unified structure, making them easy to preprocess and integrate into ML pipelines.  \n",
    "- **Large Collection:** Provides datasets for a wide range of NLP tasks, including classification, translation, summarization, and more.  \n",
    "- **Seamless Integration:** Works with `transformers` and `sklearn` for preprocessing and model training.  \n",
    "\n",
    "### **Dataset for this lab: TweetEval - Emoji Subset**  \n",
    "\n",
    "In this lab, we will work with the **TweetEval** dataset, specifically the **emoji** subset. The TweetEval dataset is a benchmark for evaluating NLP models on Twitter-related tasks, covering tasks such as sentiment analysis, hate speech detection, and irony detection.  \n",
    "\n",
    "For tokenization, we will focus only on the **text** (the content of the tweets), but we will also examine the **labels** to understand the dataset structure.  \n",
    "\n",
    "🔗 The dataset description and details are available in its dataset card: [**TweetEval Dataset**](https://huggingface.co/datasets/cardiffnlp/tweet_eval) \n",
    "\n",
    "💡 Exploring More Datasets\n",
    "Hugging Face provides a vast selection of datasets across different NLP tasks. You can browse and explore more at:\n",
    "🔗 [Hugging Face Datasets Collection](https://huggingface.co/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.531077Z",
     "start_time": "2025-04-11T09:39:54.527639Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The loaded dataset contains three subsets (“train”, “validation”, and “test”). Each consists of two columns: “text” and “label”. Label is an integer from 0 to 19 representing an emoji. See the dataset's card for more information. We can access the elements of the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.567260Z",
     "start_time": "2025-04-11T09:39:54.563839Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can easily cast the dataset to the pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.640497Z",
     "start_time": "2025-04-11T09:39:54.637846Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can plot the distribution of the labels in the training subset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.709579Z",
     "start_time": "2025-04-11T09:39:54.707375Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset's filter function\n",
    "We can filter the examples using ```filter()``` method. See this link for more details https://huggingface.co/docs/datasets/en/use_dataset. Here is an example of filtering the short tweets (less than 20 characters) from the ```train``` subset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.750019Z",
     "start_time": "2025-04-11T09:39:54.747535Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.771748Z",
     "start_time": "2025-04-11T09:39:54.769601Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Dataset's map function\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries. We will calculate the length of the text (in characters) in each example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.796619Z",
     "start_time": "2025-04-11T09:39:54.793811Z"
    }
   },
   "source": [
    "def calculate_text_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.836941Z",
     "start_time": "2025-04-11T09:39:54.832522Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can plot the histogram of the text lengths."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:54.874293Z",
     "start_time": "2025-04-11T09:39:54.871398Z"
    }
   },
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"e1\"></a>\n",
    "\n",
    "### Exercise 1: Questions about the datasets\n",
    "1. (1p) What is the size of the training, test and validation datasets? 45000, 50000, 5000\n",
    "2. (1p) What are the top 5 most frequent emojis in the validation dataset? 0, 1, 2, 7, 4, 5\n",
    "3. (1p) Compare the distributions of labels (emojis) between training and validation datasets.\n",
    "4. (1p) How many examples with the \"fire\" emoji are in the training dataset? 2146\n",
    "5. (1p) What is the average length (in characters) of the tweets in the training dataset? 71.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can add cells here to answer the questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.334510Z",
     "start_time": "2025-04-11T09:39:54.917083Z"
    }
   },
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "# from datasets import get_dataset_config_names\n",
    "# print(get_dataset_config_names(\"tweet_eval\"))\n",
    "\n",
    "train_label_counts = tweet_train_df['label'].value_counts()\n",
    "label_names = tweet_ds['train'].features['label'].names\n",
    "train_label_counts.index = train_label_counts.index.map(lambda i: label_names[i])\n",
    "print(train_label_counts)\n",
    "\n",
    "tweet_train_df.groupby('label').count().plot.bar()\n",
    "\n",
    "tweet_validation_df = pd.DataFrame(tweet_ds['validation'])\n",
    "\n",
    "tweet_validation_df = pd.DataFrame(tweet_ds['validation'])\n",
    "\n",
    "train_counts = tweet_train_df['label'].value_counts(normalize=True).sort_index()\n",
    "validation_counts = tweet_validation_df['label'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "label_dist_df = pd.DataFrame({\n",
    "    'Train': train_counts,\n",
    "    'Validation': validation_counts,\n",
    "})\n",
    "\n",
    "label_dist_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Normalized Emoji Label Distribution Across Splits\")\n",
    "plt.xlabel(\"Emoji\")\n",
    "plt.ylabel(\"Proportion of Samples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "tweet_train_df['text_length'] = tweet_train_df['text'].apply(len)\n",
    "average_tweet_length = tweet_train_df['text_length'].mean()\n",
    "print(average_tweet_length)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m### YOUR CODE HERE\u001B[39;00m\n\u001B[32m      2\u001B[39m \n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# from datasets import get_dataset_config_names\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# print(get_dataset_config_names(\"tweet_eval\"))\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m train_label_counts = \u001B[43mtweet_train_df\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m].value_counts()\n\u001B[32m      7\u001B[39m label_names = tweet_ds[\u001B[33m'\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m'\u001B[39m].features[\u001B[33m'\u001B[39m\u001B[33mlabel\u001B[39m\u001B[33m'\u001B[39m].names\n\u001B[32m      8\u001B[39m train_label_counts.index = train_label_counts.index.map(\u001B[38;5;28;01mlambda\u001B[39;00m i: label_names[i])\n",
      "\u001B[31mNameError\u001B[39m: name 'tweet_train_df' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "In this section we will preprocess the dataset by cleaning and tokenizing the entries.\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Write the text cleaning function\n",
    "\n",
    "Include at least the following steps:\n",
    "- (1p) remove comma between numbers, i.e. 15,000 -> 15000\n",
    "- (1p) remove multiple spaces\n",
    "- (1p) space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\")\n",
    "- (3x1p) three more cleaning steps of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.366322Z",
     "start_time": "2025-04-10T15:14:03.816082Z"
    }
   },
   "outputs": [],
   "source": [
    "# def clean(example):\n",
    "#     \"\"\"\n",
    "#     Cleans the example from the Dataset\n",
    "#     Args:\n",
    "#         example: an example from the Dataset\n",
    "#\n",
    "#     Returns: update example containing 'clean' column\n",
    "#\n",
    "#     \"\"\"\n",
    "#     text = example['text']\n",
    "#\n",
    "#     # Empty text\n",
    "#     if text == '':\n",
    "#         example['clean'] = ''\n",
    "#         return example\n",
    "#\n",
    "#     text = str(text)\n",
    "#\n",
    "#     ### YOUR CODE HERE\n",
    "#\n",
    "#     # 1. Remove comma between numbers (e.g., 15,000 -> 15000) -required\n",
    "#     text = re.sub(r'(?<=\\d),(?=\\d)', '', text)\n",
    "#\n",
    "#     # # Remove - for words or objects that should be treated as a single unit - optional\n",
    "#     # text = re.sub(r'\\-', '' , text)\n",
    "#\n",
    "#     # 2. remove repeated letters or punctuation to at most 2 (not numbers) to reduce variability\n",
    "#     text = re.sub(r'([^0-9])\\1{2,}', r'\\1\\1', text)\n",
    "#\n",
    "#     # 3. Replace contractions (a bit extensive i count these contractions as one and also i probably missed a couple)\n",
    "#     text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "#     text = re.sub(r\"\\b(what|who|where|when|why|how|there|it|here|she|he|that)\\'s\\b\", r\"\\1 is\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"let\\'s\\b\", \"let us\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\bi\\'m\\b\", \"i am\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\'re\\b\", \" are\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\'ve\\b\", \" have\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\'ll\\b\", \" will\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\'d\\b\", \" would\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"can\\'t\\b\", \"can not\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"won\\'t\\b\", \"will not\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"n\\'t\\b\", \" not\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"shan\\'t\\b\", \"shall not\", text, flags=re.IGNORECASE)\n",
    "#     text = re.sub(r\"\\by\\'all\\b\", \"you all\", text, flags=re.IGNORECASE)\n",
    "#\n",
    "#\n",
    "#\n",
    "#     # Unicode blabla to get rid weird spacing caused by emojis, s(for granted)\n",
    "#     text = re.sub(r'\\ufe0f', '', text)\n",
    "#\n",
    "#     # 4. removing @user since it is very frequent and not very insightful\n",
    "#     text = re.sub(r'@user\\b|(?<!\\w)@|#', '', text, flags=re.IGNORECASE)\n",
    "#\n",
    "#     # 5. Space out punctuation -required\n",
    "#     text = re.sub(r'([^\\w\\s])', r' \\1 ', text)\n",
    "#\n",
    "#     # # Reduce repeated letters (e.g., \"heelllooo\" → \"hello\") -optional\n",
    "#     # text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "#\n",
    "#     # 7. Lowercase the text\n",
    "#     text = text.lower()\n",
    "#\n",
    "#     # 8. Clean up extra whitespace (especially after spacing punctuation) -required\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#\n",
    "#     example['clean'] = text\n",
    "#     return example\n",
    "\n",
    "def clean(example):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'clean' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        example['clean'] = ''\n",
    "        return example\n",
    "\n",
    "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
    "    text = str(text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    #4 substitute by one occurance if some punction is repeated more than 4+\n",
    "    text = re.sub(r'([.,!?;:])\\1\\1\\1+', r'\\1\\1\\1', text)\n",
    "\n",
    "    # remove comma between numbers\n",
    "    text = re.sub(r'(\\d+),(\\d+)', r'\\1\\2', text)\n",
    "\n",
    "    # space out the punctuation\n",
    "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(r\"\\s+\", r\" \", text)\n",
    "\n",
    "    # three more cleaning steps of your choice\n",
    "\n",
    "    #1 lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #2 remove #, @, *, ^, () , {} , ...\n",
    "    text = re.sub(r'[@#*^(){}\\'\\\"%\\$&\\[\\]]', r\"\", text)\n",
    "\n",
    "    #3 delete the \"-\" between words\n",
    "    text = re.sub(r'\\-', r\"\", text)\n",
    "\n",
    "    # remove the initial and final spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # reinforce the previous ones\n",
    "    text = re.sub(r\"([.,!?;:])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\" ️\", r' ', text)\n",
    "    text = re.sub(r\"\\s+\", r' ', text)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    # Update the example with the cleaned text\n",
    "    example['clean'] = text.strip()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is an example of applying the ```clean()``` function you just wrote to a single entry of the dataset. The function added a 'clean' field to the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.404900Z",
     "start_time": "2025-04-10T15:14:04.029385Z"
    }
   },
   "source": [
    "print('Original tweet item:')\n",
    "print(tweet_ds['train'][2]['text'])\n",
    "print('Cleaned tweet item:')\n",
    "print(clean(tweet_ds['train'][2])['clean'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet item:\n",
      "Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…\n",
      "Cleaned tweet item:\n",
      "love love love all these people friends bff celebrate blessed sundayfunday san …\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. You can see that the ```clean``` column has been added to each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below, we will apply your function to all entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.408574Z",
     "start_time": "2025-04-10T15:14:04.241791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45000/45000 [00:03<00:00, 11670.59 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:03<00:00, 14451.49 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 13650.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Build vocabulary\n",
    "\n",
    "In the previous section, we implemented the cleaning of the dataset. Now, we will tokenize the text splitting it by spaces. We will build a vocabulary based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law).\n",
    "\n",
    "The function below builds a vocabulary from the dataset. It counts the occurrences of the words in the dataset using the Counter class. Check the documentation here [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3: Build the vocabulary\n",
    "(5p) Fill in the function below to build the vocabulary from the dataset. The function should return a `Counter` object with the words and their frequencies. The variable named `vocab` is already initialized as an empty `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.415391Z",
     "start_time": "2025-04-10T15:14:12.435513Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    for tweet in dataset:\n",
    "        text = tweet['clean']\n",
    "        vocab.update(text.split())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.416014Z",
     "start_time": "2025-04-10T15:14:12.648516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 53937\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because we created a counter, we can easily check the most and least common words in the vocabulary. Do the most common words make sense? How about the least common ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.416183Z",
     "start_time": "2025-04-10T15:16:39.617399Z"
    }
   },
   "source": [
    "print('Most common:')\n",
    "print(vocab_counter.most_common(20))\n",
    "print('Least common:')\n",
    "print(vocab_counter.most_common()[-20:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common:\n",
      "[('.', 19639), ('…', 19238), ('!', 14981), ('the', 13878), (',', 12382), ('i', 8553), ('to', 7843), ('my', 7655), ('a', 7025), ('in', 6103), ('you', 5889), ('is', 5828), ('and', 5799), ('with', 5278), ('of', 5194), ('for', 4876), ('this', 4626), ('california', 4491), ('at', 3739), ('it', 3613)]\n",
      "Least common:\n",
      "[('bluewall', 1), ('skyshow', 1), ('oneblockaway', 1), ('convoy', 1), ('whatshotontheblock', 1), ('mountaingirl', 1), ('traivs_', 1), ('hangz', 1), ('bæ', 1), ('squints', 1), ('corban', 1), ('southbayla', 1), ('thedabberchick', 1), ('nector', 1), ('chefking1921express', 1), ('rolltide', 1), ('bffweekend', 1), ('nunez', 1), ('happylaborday', 1), ('five50', 1)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.417782Z",
     "start_time": "2025-04-10T15:14:14.073891Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.loglog([val for word, val in vocab_counter.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('count')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAG1CAYAAADwRl5QAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPSRJREFUeJzt3Ql0ldX19/FfbkYCYQiBBEggzBCGoExGUYGigArFkaoVRIsVEa1oFWsL0mp5q6JWpVWhlKHa8ncAFBQRBFFkDiDIGCHMJIyZIIQkvOscyBUENGR67vD9rHUX9zz3ctk8YrJzzt7nBJw6deqUAAAAfJDL6QAAAADKC4kOAADwWSQ6AADAZ5HoAAAAn0WiAwAAfBaJDgAA8FkkOgAAwGeR6AAAAJ8VJD9XWFiovXv3KiIiQgEBAU6HAwAAisHsd5yVlaW6devK5br4vI3fJzomyYmLi3M6DAAAUAK7du1SbGzsRV/3+0THzOQU3aiqVas6HQ4AACiGzMxMO1FR9H38Yvw20Rk3bpx9FBQU2LFJckh0AADwLj9XdhLg74d6moywWrVqysjIINEBAMDHvn/TdQUAAHwWiQ4AAPBZfpvomPqchIQEdezY0elQAABAOaFGhxodAAC8DjU6AADA75HoAAAAn+W3iQ41OgAA+D5qdKjRAQDA61CjAwAA/B6JDgAA8FkkOgAAwGf5baJDMTIAAL6PYmSKkQEA8DoUIwMAAL9HogMAAHwWiQ4AAPBZfpvoUIwMAIDvoxiZYmQAALwOxcgAAMDvkegAAACfRaIDAAB8FokOAADwWSQ6AADAZ/ltokN7OQAAvo/2ctrLAQDwOrSXAwAAv0eiAwAAfBaJDgAA8FkkOgAAwGeR6AAAAJ9FogMAAHwWiQ4AAPBZfpvosGEgAAC+jw0D2TAQAACvw4aBAADA75HoAAAAn0WiAwAAfFaQ0wF4ir/M+k6h4VWK9d4ABRTrfVXCgtQurrour19DtSJCSxkhAAC4VCQ6Z0xbsVuu0PBy+/y4yEpqX7+GLm9QwyY+LWIiFBTIhBoAAOWJROeMh7o2Vljln5/RuZQetfSsXCXvOKot6Vnadfi4fcxYs9e+Vik4UIlx1WzSYx8Naiiyckhp/goAAOBHaC+vgPbyzNyTWrPzqJJ3HlHyzqNavfOIsnLzz3tfw6jKuqx+dbU/M+vTLDpCga7iLZMBAOBPMov5/ZtEx4F9dAoLTynlQLaSdxxxJz8p6dnnva9KaFGNT3VdZpKfuBqqFh5cITECAODJSHS8bMPAo8fytHrXUa3ecUSrdh6xM0A5eQXnva9J7So28TEzPmbmp3GtKnIx6wMA8DOZJDrelej8WEHhKW1Jy7IzPqt2HNHqnUe1/WDOee+rajq7bJ3P6SWv+pHhtv4nNDjQ/hocGKCAABIhAIBvIdEpxllX5lFQUKAtW7Z4XKJzIYdz8mx9j0l8TAK0dleGjp88f9bnbKbGJyzIpUohgQoLPv0wCVBYsOus56d/Ne8JDwnUDW3qqHW9ahX29wIA4FKR6Hj5jE5x5BcUatP+07M+pt7HLH0dzDphk5/CUvxXNSth93dpqOHXNbfJDwAAnoZExw8SnYsx/0lPFpyyCU/umcfp54U6nnfutaLrdpxXYJfL5m5Is59jlsHG3NJGVzWJcvqvBABAib5/s4+ODzI1OSFB5uFStUqX3qX1xaY0/XH6eu08fEx3T1imOzrE6pkbEuj4AgB4HbbmxXm6t4jW3OHXakBSAzv+v5W71eOVL/Xpun1OhwYAwCUh0cEFmT18/vzL1nr/wSQ1rlVZB7JOaMg7yfrt1JVKz8x1OjwAAIqFRAc/qUN8pGY/crUe7tZEQa4AffZdmn7x8pf63/KdthYIAABPRqKDn2Xaz5/o2VwfD+uitrHV7PEVIz5cp7vGL1PqBfb2AQDAU9B15YNdV+Xd0v7vxaka+/lm260VGuTS9a1ilNSoppIa11R8zXA2KAQAlDvay4uJRKdkdhzK0R+mr9PilEPnXI+pGmYTnisaRSqpUZTiIiuR+AAAyhyJTjGR6JSc+aezIvWIFqcc1JJth+z5XHkFhee8p171SrqiUU3d0CZG3ZrX5lwuAECZINEpJhKdsmM2HDS7NC/5/pCWmsRn11Hln7VFc9PaVfTbaxurb2Jdu8cPAAAlRaJTTCQ65edYXr5Wph7Rws0H9N7KXco6kW+v16kWZo+YuLNTfVUOZc9KAMClI9EpJhKdipGZe1LvLN2piYu32z15DLNr8z1XNNC9V8Urqkqo0yECALwIiU4xkehULHOm1vTVe/T2om3afqY13ZycPvX+TnbPHgAAyvL7N4USqPA9ecyS1bzh1+qfd1+uhDpV7cGif5r5nQpKc+Q6AAC+nOgcO3ZMDRo00BNPPOF0KCiGQFeAerepo3d+01lVw4K0cV+mPli12+mwAAA+xmcSneeff15XXHGF02HgEtWoHKJHftHUPn9x7mblnClYBgCgLPhEorN161Zt2rRJvXv3djoUlMA9SQ3UoGa4LVJ+68vvnQ4HAOBDHE90Fi1apD59+qhu3bp2B90ZM2ac955x48YpPj5eYWFh6ty5s5YvX37O62a5asyYMRUYNcpSaFCgnu7d0j5/+6tt2nv0uNMhAQB8hOOJTk5OjhITE20ycyHTpk3T8OHDNWrUKCUnJ9v39uzZU+np6fb1mTNnqlmzZvYB79WzVbQ6NYy052e99Nlmp8MBAPgIj2ovNzM606dPV79+/dzXzAxOx44d9cYbb9hxYWGh4uLiNGzYMI0YMUJPP/20/vOf/ygwMFDZ2dk6efKkHn/8cY0cOfKCf8aJEyfs4+z2NPN5tJc7b93uDPV542v7/KOHr1Lb2OpOhwQA8FA+0V6el5enVatWqUePHu5rLpfLjpcsWWLHZslq165dSk1N1UsvvaTBgwdfNMkper+5MUUPk+TAM7SJraZbLq9nnz83a6M9SwsAgNLw6ETn4MGDKigoUHR09DnXzXj//v0l+kwzA2Syv6KHSZLgOX7fs7nCgl1annpYn31Xsv/GAAAU8amDhu69996ffU9oaKh9wDPVqVZJD1zTWK/N36q/frJJ3VrUtsXKAAD43IxOVFSUrb1JS0s757oZx8TElOqzTfFzQkKCrf+BZ/ntNY1UOyJUOw8fs4XJ5tgIAAB8LtEJCQlR+/btNX/+fPc1U4xsxklJSaX67KFDh2rDhg1asWJFGUSKsmRONH+iZ3P7fPxX23XNCws04att9jR0AAC8aunKdEqlpKS4x9u3b9eaNWsUGRmp+vXr29bygQMHqkOHDurUqZNeffVV25I+aNAgR+NG+bq9faxOFhRq3Bcp2puRq+dmb9Q/Fn6v+7s0tBsMVg0LdjpEAIAXcLy9fOHCherWrdt5101yM2nSJPvctJa/+OKLtgC5Xbt2eu2112zbeWmXrszDFDtv2bKF9nIPlZdfqOmrd9skZ8ehY/ZaaJBLjWpVUaNaldUoqrL91bSiN65VxelwAQAe1l7ueKLjLTcKzsovKNTsdfv0xhcp2pqefcH39O8Qp6d6t1Bk5ZAKjw8AULFIdIqJRMe7mH+uqYeOaduBbG07kKNtB7OVkp6tFalH7OvVw4P1ZM8W+lXHOLlcAU6HCwAoJyQ6xUSi4xtWph7WH2es16b9WXacGFddr/Zvp4ZRlZ0ODQBQDnxiZ2SguDrER2rWsC4aeVOCqoQGae2uo/r1hGXal8EBoQDgz/w20WEfHd8TFOjSfV0aav7j19oC5T1Hj2vAv5brSE6e06EBABzC0hVLVz7JJDm3/uMb7c/M1WX1q+ud33RWeIjjuykAAMoIS1fwa/WqV9LU+zupWqVgrd55VEP+k6ys3JNOhwUAqGDM6DCj49NW7Thia3WOnyxQcGCArmhUU9cnRNtfo6uFKSI0SAEBdGcBgLeh6+pnsGGg/1icclB/mrFe2w7mnPeaOSk9qkqoLWA2j9pVQ/XoL5qpeUyEI7ECAIqHRKeYmNHxH98fyNbnG9I0b0OaNqdlKSv3wmdnmQ0H/zv4CpIdAPBgJDrFRKLjv47nFSg9K1cHs/OUcyLfPv755ff6dneGalYO0f8euEJNo0l2AMATkegUE4kOzpZx7KTu/tdSrd+TqRrhwereIlot60To2ma1SHoAwIOQ6BQTiQ5+7OixPN09YZm+25vpvhbkCtDTN7TUfVfFU7wMAB6ARKeYSHRwIbknC7RoywFt2JepZdsOa8m2Q/b6jW3q6P/d2kYRYcFOhwgAfi2TROen0XWF4jL/i0z+JlXPzd6o/MJTdo+el25PVFLjmk6HBgB+K5NEp3iY0cGl7Mnz6P9Wa/eR0+dn3Xp5rO7sFKf2DWqwnAUAFYxEp5hIdHApsk/k6/nZG/Xf5Tvd1zrG19DU+zsrLDjQ0dgAwJ9kcgQEUPbMpoJjbmmjD4Zcqdvax9oNB1ekHtGc9fudDg0AcAEkOkAJmOUqU6fz4LWN7fiD5N1OhwQAuAASHaAUTJ2O8XXKQe09erp2BwDgOUh0gFKIiwxX54aRMpVu01fvcTocAMCP+G2iY1rLExIS1LFjR6dDgZe7tf3pWZ33V+22regAAM9B1xVdVyiDTqyOz83T8ZMFurpplIZ1b6rEuGoKDaILCwCc/v4dVG4RAH7UifVkr+a27fyrrQftI9AVYNvOx911uWpWCXU6RADwW367dAWUpUFXNdQXj3e1LedVw4JUUHhKS7cd1ogP17GcBQAOItEBykj9muG25XztqOv14UNXKjgwQJ9vSNOkb1JVWEiyAwBOINEBypg5DuLy+jX0ZM8Wdjz64w3q+Pw8fbCKvXYAoKKR6ADl5P4uDXXvlfGqHBKoQzl5+uOM9co4dtLpsADAr5DoAOXE5QrQs31bac2o69UiJsJ2Zb23apfTYQGAXyHRAcpZcKBLA5Li7fMpS3bYQmUAQMXw20SHDQNRkfpdVtd2Y+08fEx//WSj0jJznQ4JAPwCGwayYSAqyCufb9Hf52+1zysFB9oOrRvb1nE6LADw6e/ffjujA1S0R3/RVK/deZkSY6vZep2h7ybrb3M2sZQFAOWIRAeowOLkvol19cGQK/XANY3stX8u/F73T16hvPxCp8MDAJ9EogNUsKBAl/5wQ0v9/VftFBbs0sLNBzRlSarTYQGATyLRARzyy3b19Oe+re1zU7uz6/AxdlAGgDLGoZ6Ag25tH2uPiNiwL1NXv7DAdmZd3bSWrm1WS7WrhqpZdITqVq/kdJgA4LXouqLrCg5L3nlED05dpfSsE+e9Zs7Luu+qhnrsumYKCw50JD4A8Obv3yQ6JDrwECcLCvXt7gx9ueWAlm47pEPZJ/T9gRz7Wut6VfX/bmmr1vWqOR0mAHgEEp1iItGBJzOnnz/5/lodOXZSAQHSze3q6ekbWqpWRKjToQGAo9hHB/AB1yVE65NHr7Zt6eZHkg9X79HgKSvl5z+fAECxkegAHq5OtUp2o8EZQ69SeEig1uw6ak9CN11aAICf5reJDmddwdu0i6uu31x9eqPBd5bttF1apoj5cE6e06EBgMeiRocaHXiRnBP5GvHhOm07kG1b0s3/vWaW59dXNNDTvVsowBTyAIAfyCzm92/20QG8SOXQIL1+52X2+Xd7MzR82lptTsvS24u22WsPdW2s6uEhDkcJAJ6DGR1mdODFzP++Exen6i+zNriv9WoVY2t6QoL8dmUagB/IpOsK8H1mqer+Lg31wm1tFV31dMv5nO/2a9Ck5fp291GnwwMAxzGjw4wOfIT5X3n+xnQ99E6y8gpOn4Y+5pY2urNTfadDA4Ayx4wO4IezOz0SovXp767WVU1q2msvzNmkmWv22F2WAcAfkegAPqZxrSqaPKiTGteqbHdUfvR/a9R97Jdau4ulLAD+h0QH8EFBgS5Nvq+T7rmigeJrhivj+En1+8diPfvRd/r+QDY7KwPwG9ToUKMDH5dx7KSeeH+tPTerSGJsNT1wTWMl1K2qhlGVHY0PAEqCQz2LiUQH/mLO+n12v53Vu47ajQaLDLoqXqP6tHIyNAC4ZGwYCOAcvVrXsY+U9Gy9Mm+Ldh46pnV7MvTvxalqHh2hX9GdBcAHMaPDjA782LgFKXrxs81yBUjDr2ume5LiVa1SsNNhAcDPor0cwM8acm1j9WgZrcJT0ktztyhx9FyNmrle+Wf24QEAb0eiA/gxlytAL/dP1OCrG6pe9Ur22uQlO/SbKSvpzALgE0h0AD9XNSxYz9yYoK+f6mbPyAoODNDCzQf0+HtrVWCmegDAi3l9onP06FF16NBB7dq1U+vWrTV+/HinQwK8dmflvol19cT1ze34w+Q9mrfxh5Z0APBGXt91FRERoUWLFik8PFw5OTk22bnllltUs+bpLfABXJrfXttYW9Oz9f6q3Zr49Xb1bBXjdEgA4L8zOoGBgTbJMU6cOGHrCqgtAErn8eubKdAVoGXbD9uzsgDAWzme6JjZmD59+qhu3bp26nzGjBnnvWfcuHGKj49XWFiYOnfurOXLl5+3fJWYmKjY2Fj9/ve/V1RUVAX+DQDfU6daJd3ZKc4+N2dldfnbFxr98XfKPVngdGgA4F2JjlluMkmKSWYuZNq0aRo+fLhGjRql5ORk+96ePXsqPT3d/Z7q1atr7dq12r59u959912lpV28rsDM+pje+7MfAM43um9r241l9tjZfeS43VjwsWlrlJdP6zkA7+FRGwaaGZ3p06erX79+7mtmBqdjx45644037LiwsFBxcXEaNmyYRowYcd5nPPTQQ+revbtuu+22C/4Zzz77rEaPHn3edTYMBC4sM/ek5m9M0+/f+1b5had0ddMojR/QQWHBgU6HBsCPZfrChoF5eXlatWqVevTo4b7mcrnseMmSJXZsZm+ysrLsc/OXNUthzZuf7hq5kKefftq+r+ixa9euCvibAN7dfn7zZbF6467LFR4SqK+2HtQfPlzndFgA4P1dVwcPHlRBQYGio6PPuW7GmzZtss937NihBx54wF2EbGZ62rRpc9HPDA0NtQ8Al6ZX6xhVDeuguyYs04er9+hA9gn96aYENYuOcDo0APDORKc4OnXqpDVr1jgdBuAXrmwSpYe7NdE/FqbYmZ3rX1mkzg0jNeX+TgoNYikLgOfx6KUr0z1l2sd/XFxsxjExpdvbwxQ/JyQk2PofAMX3RM/mWvBEV3VoUMOOTQv6i3M2Ox0WAHhfohMSEqL27dtr/vz57mumGNmMk5KSSvXZQ4cO1YYNG7RixYoyiBTwLw1qVtZ7DyZpdN9Wdjzh6+36/kC202EBgOclOtnZ2XbpqWj5ybSIm+c7d+60Y9Nabo51mDx5sjZu3KghQ4bYlvRBgwY5HDng30yX5MAr45UYW82OfzH2S72zbIfTYQGAZ9XorFy5Ut26dXOPTWJjDBw4UJMmTVL//v114MABjRw5Uvv377dnWs2ZM+e8AuWSLF2Zhyl2BlByD3dvqpEz12tfRq6emb5e2w/k6KFuTRRZOcTp0ADAs/bR8eQ+fAAXV1h4yu6cPHnJ6RmdZtFV9NnvrrGzPgBQHnxiHx0A3sHlCtDIPq3015vbKCTQpS1p2UreedTpsACARAdA2TCHgN7Vub66Nq9lx2aGx8z0AICT/DbRob0cKB/P9m2lIFeAvt2doR4vf6k1u5jZAeAcanSo0QHKnOm++uvsjcrJK1DtiFDNfewaVQ+nOBlA2aFGB4Bj7u7cQItHdFejqMpKzzqh295con0Zx50OC4AfItEBUC7MDM7YOxJVKThQKenZ9riIFz/bpBP5bOkAoOKwdMXSFVCuzI7JQ99J1qb9WXZcs3KIereJ0b1XxqtJbQ4EBVAyLF39DIqRgYrRuFYVffLI1RpzSxuFBLl0KCdP/1m6U4//31qnQwPgB5jRYUYHqDD7M3I169u9em72RgUHBui70b1s8gMAl4oZHQAeJ6ZamO7v0tAeD3Gy4JTGLUhxOiQAPo5EB0CFMsdC/PHGlvb53+dv1evztzodEgAfRqIDoMLdcnmsBiY1sM9fX5CinYeOOR0SAB9FogPAsR2UW8REKC+/UDe+/pXWsoMygHLgt4kOXVeA80tY4+6+XHWqhSkrN18PTF2pwzl5TocFwMfQdUXXFeCo7Qdz1Pvvi5R7slDNoyM09TedVDsizOmwAHg4uq4AeIWGUZU19f7OdgflzWlZ+uUbizVzzR75+c9gAMoIiQ4Ax3WMj9R7DybZs7H2ZeTq0f+t0a/eXqpdhylSBlA6JDoAPELretU0fehVuuWyena8bPthXf3CAn255YDToQHwt0Sne/fuOnr06AXXy8xrAFAS1SoF6+X+7fSvgR3c1wZOXK5fT1im/IJCR2MD4EeJzsKFC5WXd353RG5urr766it5A7quAM/1i5bR+urJbrqxbR07/jrloHq+ukib9mc6HRoAX+66+vbbb+2v7dq10xdffKHIyEj3awUFBZozZ47eeustpaamylvQdQV4tn8sTNHYuVtUUHj6S5U5QmL4dc1UOTTI6dAAeMH370tKdFwul937wrjQb6tUqZJef/113XffffIWJDqA59ualqUbXvvKno9lVAkN0s2X1dOjPZoqqkqo0+EB8JVEZ8eOHTbBadSokZYvX65atWq5XwsJCVHt2rUVGBgob0KiA3iH3JMF9hDQj9fuVeqZIyPiIivps99do/AQZncAf5NZHomOLyLRAbxLYeEpfbJ+nx5+d7UdN6ldRW/d016Na1VxOjQAHvj9u8Q/Bm3dulULFixQenq6CgvP7YYYOXJkST8WAH6SyxWgm9rWtctXQ/6TrJT0bP1i7Jca2q2xHuvRTEGB7JoBoJQzOuPHj9eQIUMUFRWlmJgYd92O/cCAACUnJ8tbMKMDeK/1ezI09N1k7TizlPWLFrX12p2XUagM+IHM8ly6atCggR566CE99dRT8nYkOoD3L2WZ2p3XF6TYk9Crhwfr8eub665O9RXo+uGHMAC+pVwTHfOBa9assUXJ3o5EB/ANK1MP66F3kpWedcKOm0VXsQnPdS2j7XIXAN9Srod63n777Zo7d668GRsGAr6lQ3ykFj3ZTb+9ppFCglzakpat305dpU5/nWdnfDKOnXQ6RAAOKNGMzpgxY/Tyyy/rxhtvVJs2bRQcHHzO64888oi8BTM6gO85lH1CExdv1+Rvdij7RL69ZvbbGXNLG12XEO10eAA8femqYcOGF//AgABt27ZN3oJEB/BdJwsK9dGavRq3MEXbDuTYmp3xA9qrewuSHcDbsY9OMZHoAP6x2eCT73+rj9buteOuzWvpmRtaqml0hNOhAfDEGh0A8CZhwYF64ba2uqNDrB0v3HxAvxy3WGmZuU6HBqCclWhG5+fOspo4caK8BTM6gH/ZkpalX729VIdz8tQurrr+enMbJdTl/33A25TrjM6RI0fOeZjdkc1p5h9++KGOHj1amrgBoFw1i47Qs31b2edrdh21h4XOXLPH6bAAlJMSbR86ffr0866ZYyDMbsmNGzcui7gAoNz0Tayr+JrhGvbf1XZX5Uf/t0axNcLVvkENp0MDUMbKrEbH5XJp+PDheuWVV8rqIwGg3LSNra4p93VS0JnNBG/95zeavzHN6bAAlLEyLUb+/vvvlZ9/es8KAPB0DWpW1tpR1+uy+tXt+P7JKzVy5nrblg7Aj5euzMzN2Uw98759+zR79mwNHDiwrGIDgHJnDgAdP6CD+r7+tfZm5GrKkh2anrxH3z57/TkHFgPwo66rbt26nbdsVatWLXXv3t12ZAUFec/JwXRdATByTuRr0L9XaHnqYTvu1SpGb9x1mYIC2YUD8ERsGFhMJDoAipgvh6ZAeda3++y4U3ykxt19uWpFhDodGgAnNgw8cOCAvv76a/swz70Jh3oC+DGzVPXGXZfrtTsvs8dFmNmdjs/Po24H8GIlmtHJycnRsGHDNGXKFNtWbgQGBmrAgAF6/fXXFR4eLm/BjA6AC1mReliP/He19mWc3j3ZHAY67q7L7cnoAHx8RscUI3/55Zf6+OOP7QaB5jFz5kx77fHHHy9N3ADgETrGR+qbEd1131WnDzH+fEOabnztK2UcP+l0aADKe0YnKipK77//vrp27XrO9QULFuiOO+7wqmUsZnQA/JypS1L1p5nf2edmRmfs7Ym6qW0durIAX53ROXbsmKKjo8+7Xrt2bfsaAPiSe5Li9Ur/RPs8L7/QFiz3/vtX2nEox+nQAPyMEiU6SUlJGjVqlHJzfzj59/jx4xo9erR9DQB8zc2XxWrNyOt0Y5s6drxpf5a6j/1SCzanOx0agLJeulq3bp169eqlEydOKDHx9E85a9euVWhoqObOnatWrU4fmOcNWLoCcKlW7Tis305N1sHsE3b8hxta6IFrOOcP8Kl9dMwS1TvvvKNNmzbZccuWLXX33XerUqVK8iYkOgBKIiv3pHq9+pX2HD1uxyNvStB9XU4XLgPw8kRnzJgxtkbH7IJ8tokTJ9pC5KeeekregkQHQEkdzyvQwH8v1/Ltp3dTvqtzfT3frzVFyoC3FyO/9dZbatGixXnXzZLVm2++WZKPBACvUykkUO/8prOa1K5ix+8u26k73lpid1gG4BlKlOjs379fdeqcLsg7mznvyhzuCQD+IjjQpc8fu0a3tY+14xWpR3TrP7/RtgPZTocGoKSJTlxcnBYvXnzedXOtbt26ZREXAHgNs1T10u2JGta9iR0n7zxqO7IemLJS+RwdAXhfojN48GD97ne/07///W/t2LHDPkx9zmOPPWZfAwB/9Pj1zfX3X7VTZOUQO567IU1NnvlUW9KynA4N8FslKkY2v2XEiBF67bXXlJeXZ6+FhYXZIuSRI0fKm1CMDKCsma+R901aoQWbf9glvm1sNb11T3vVqeZdnamA37aXG9nZ2dq4caNtKW/atKndR8fbkOgAKC+mOPkP09edc232I13Uqm41x2ICfEW5dl0VqVKlijp27KjWrVs7luTs2rXLnrmVkJCgtm3b6r333nMkDgD4MdNuvukvvXRnp/ruaze+9rUWbGI3ZaCilGpGxxOYLq+0tDS1a9fOdoO1b99eW7ZsUeXKlYv1+5nRAVAR5m1I02+mrHSP770yXs/29Z5d5AG/nNHxBKbN3SQ5RkxMjD1Z/fDh05t3AYCn6JEQbU89LzLpm1TNWb/f0ZgAf+B4orNo0SL16dPHtqWbFs0ZM2ac955x48YpPj7eFjx37txZy5cvv+BnrVq1SgUFBbb9HQA8za3tY7Xu2evd4wf/s0pfbf2hYBmADyY6OTk59mBQk8xcyLRp0zR8+HB7WnpycrJ9b8+ePZWefu4at5nFGTBggN5+++0KihwALl1EWLDdYLDIPf9artU7jzgaE+DLPKpGx8zoTJ8+Xf369XNfMzM4puD5jTfesOPCwkI7YzNs2DDb4m6YU9Svu+46u4fPPffc85N/hnmveZy9xmc+jxodABVp2bZD6v/2Uvf4+Ztb6+7ODRyNCfAmPlGjY/boMctRPXr0cF9zuVx2vGTJEjs2edq9996r7t27/2ySU3QgqbkxRQ+WuQA4oXOjmnrz1+3d42emr9efZqx3NCbAF3l0onPw4EFbc2NOSj+bGZsOq6JjJ8zylqntMUXJ5rFu3bn7Vpzt6aefttlf0cO0pwOAE3q1jtG84T8sY01dusMeG+FBE+2A1wuSl+vSpYtdzious9+PN25sCMA3NakdYQuU2zw7131sRMOnP9Hcx65Rs+gIp8MDvJ5Hz+iYVvHAwEC7T87ZzNi0kpeGKX42mwya+h8AcLpAeeOfe51z7fpXFulfX293LCbAV3h0ohMSEmI3AJw/f777mpm9MeOkpKRSffbQoUO1YcMGrVixogwiBYDSqRQSqO1jbtDD3U6fgG78ZdYGTSTZAbw70THnZa1Zs8Y+jO3bt9vnO3futGPTWj5+/HhNnjzZnqs1ZMgQ25I+aNAghyMHgLLvPH2iZ3PNGtbFfe3PszZoz9HjjsYFeDPH28sXLlyobt26nXd94MCBmjRpkn1uWstffPFFW4Bsio3Nqemm7bwscAQEAE+089AxXfPiAvf45TsSdcvlsY7GBPjd6eXezNTomIfp6jJnY5HoAPA0s77dq4ffXe0ev3BrW93RkS0xAINEp5iY0QHgydbvydBNr3/tHj/du4V+e21jR2MCPIFPbBgIAP6udb1q+uSRq93jMZ9u0gtzNjkaE+BNSHQAwMMl1K2qFc/8sEP8PxZ+r/sn0TEKFIffJjrsowPAm9SKCNXqP13nHs/flK67JyxVYaFfVx8AP4saHWp0AHiR/IJCNXnm03OumZ2VzaaDgD/JpEYHAHxPUKBLG/7cU2HBP3z5NsdH+PnPrMBFkegAgJcJDwnShtG91K15Lfe1fuMWk+wAF0CiAwBeyOUK0L8HdXKP1+7OsIeBZhw/6WhcgKfx20SHYmQAvmDzc+ceBpo4eq5yTxY4Fg/gaShGphgZgJcznVe//c8qfb4hzX0t5fnetp4H8FUUIwOAHy1jjR/QQZ0bRrqvmc6sAlrPARIdAPAV/x18hRrVquweN/7DJ1q27ZCjMQFOI9EBAB+a2Zk//Fq1rvfDNH7/t5dqypJUR+MCnOS3iQ7FyAB8UUBAgGYNu1rP39zafW3kzO/0YfJuR+MCnEIxMsXIAHxUSnqWery8yD1+d3BnXdk4ytGYgLJCMTIA+LkmtSP02e+ucY/vGr9Ms7/d52hMQEUj0QEAH9Y8JkJT7/9hY8Gh7yZr3e4MR2MCKhKJDgD4uKub1tJHD1/lHvd542t9k3LQ0ZiAikKiAwB+oG1sdb11T3v3+K4JyzRtxU5HYwIqAokOAPiJnq1izlnGeuqDdXr2o+84DBQ+zW8THdrLAfjrMta84T8UKE/6JlX3T15JsgOfRXs57eUA/FDGsZNK/PNc93hgUgON/uUPe+8Ano72cgDARVULD9aakde5x5OX7NBLn212NCagPJDoAICfqh4eosUjurvHbyxI0ZPvr3U0JqCskegAgB+rV72Svn6qm3v8fyt36+4JS5VfUOhoXEBZIdEBAD8XWyNc60f3VEDA6fHilENq8syn2nEox+nQgFIj0QEAqEpokFKev0EJdX4o6rz2xYU6lH3C0biA0iLRAQBYga4AzX6ki/7yy1bua+2fm6fdR445GhdQGiQ6AAC3gIAA3ZMUr8d6NHNf6/K3BVqZetjRuICS8ttEhw0DAeDiHu3RVE/2au4e3/bmEi3ddsjRmICSYMNANgwEgIv6aO1ePfLf1e7xH29sqd9c3cjRmACDDQMBAKXWN7Gu3vz15e7xc7M3aurSHY7GBFwKEh0AwE/q1bqO5j72w/lYf5qxXq/O2+JoTEBxkegAAH5Ws+gIffTwVe7xq/O2asJX2xyNCSgOEh0AQLG0ja1+TrJjlrE+XrtXhYV+XeoJD0eiAwC4pGTnvQeT3ONh/12t/63Y5WhMwE8h0QEAXJKO8ZEae3uie/yH6et0zQsLlJaZ62hcwIWQ6AAALtmt7WM19f5OdjdlY+fhY7r+lUXatD/T6dCAc5DoAABK5OqmtZT8x+t0ddMoO844flKDp6zU0WN5TocGuJHoAABKrFp4sF3GSmpU0453HT6uK8bM14EsDgOFZyDRAQCUSu2qYXprQHslxlW349yThXr0f6u1fDvnY8F5fpvocNYVAJSdqmHBmjn0KvVoGW3H33x/SH+bs0kHs5nZgbM464qzrgCgzKRn5urf36Tqnwu/d18b0rWxnurVwtG44Hs46woA4Mgy1sPdmqhp7So605ClWd/u1bvLdior96TT4cEPkegAAMpU5dAgfT78Wr0/5Ep3gbLZa2f8V9udDg1+iEQHAFAu2sVW1yO/aKrW9U4vK3ywarcemLJSn29Iczo0+BESHQBAuXC5AjT8umYacm0TO95z9LjmbkjTmE83Oh0a/AiJDgCgXPVsFa03f93eJj3GjkNmF+UvNXbuZqdDgx8g0QEAlKugQJd6tY7R/V0aqkpokAoKT2lLWrbGLUjh5HOUOxIdAECFFSnPG36tJt/XyY5NjtN97ELd8eYSpWdxICjKB4kOAKDCxFQL07XNaim+Zrgdpx46puWph/Xl5gNOhwYfRaIDAKhwHw3rov/7bZKubHz6jKw3v/xe901aoVU7ODYCZYtEBwDgyJERnRpGus/H+v5Ajr7YlK7xi9hrB2UrqIw/DwCAYnuke1MlxlazZ2NNWbJDm9Oy9Pai7xVZOVS/bFdXwYH8PI7SIdEBADimUkigerWuo/CQIJvobD+Yo79+ssm+Fh4SqBva1HE6RHg5Eh0AgOOSGte0Z2TtzTiuFamH7bER+zPoxELpcXo5p5cDgEcZ8cG3+t+KXe6xOSD0o4e72NkfwC9PL7/55ptVo0YN3XbbbU6HAgAopSsa1XSffG5sTc9WSnq2kyHBi/lEovPoo49qypQpTocBACgD/S6rpzWjrtfKP/Zw77ezcHO6Pl23T+v3ZDgdHryMT9TodO3aVQsXLnQ6DABAGbaf218rnf517Odb3K8tfKKr4qMqOxYbvIvjMzqLFi1Snz59VLduXQUEBGjGjBnnvWfcuHGKj49XWFiYOnfurOXLlzsSKwCgYj3UtYmuaBSpTvGRtgvL2H3kuNNhwYs4nujk5OQoMTHRJjMXMm3aNA0fPlyjRo1ScnKyfW/Pnj2Vnp5e4bECACqWOQz0fw8k6f8eTFKz6Ah77bPv9uvdZTs1c80e5Z4scDpEeDjHl6569+5tHxfz8ssva/DgwRo0aJAdv/nmm5o9e7YmTpyoESNGXPKfd+LECfs4u2obAOD5IsJOf8uaunSH+9qfbkqwp6IDHjuj81Py8vK0atUq9ejRw33N5XLZ8ZIlS0r0mWPGjLHtaEWPuLi4MowYAFBeHvlFU93Ypo6uT4hWwzM1OvszWMaCFyc6Bw8eVEFBgaKjo8+5bsb79+93j03ic/vtt+uTTz5RbGzsTyZBTz/9tO25L3rs2vXDXg0AAM/VMT5S4+6+XG8P6KC+iXXttf2ZJ7Q1LUs7DuXIz7eFg6cuXZWFefPmFfu9oaGh9gEA8F5Fmwd+vHavfRhPXN9MD3dv6nBk8DQePaMTFRWlwMBApaWlnXPdjGNiYkr12ab4OSEhQR07dixllACAitateW01iqqsyMoh7m6s7/ZScwkvS3RCQkLUvn17zZ8/332tsLDQjpOSkkr12UOHDtWGDRu0YsWKMogUAFCRmsdE6Isnuir5T9fp2T6t7DU6sOCRS1fZ2dlKSUlxj7dv3641a9YoMjJS9evXt63lAwcOVIcOHdSpUye9+uqrtiW9qAsLAODfQoNP/8y+eX+Wnpu1wT6PCAvWwCsbqHp4iMPRQf6e6KxcuVLdunVzj01iY5jkZtKkSerfv78OHDigkSNH2gLkdu3aac6cOecVKJdk6co8TLEzAMB7meUrY29GriZ8vf2cBOjBaxs7GBk8AaeXc3o5AHi1/IJCu7fO/sxcO176/SGt3Z1hk5wRvVs4HR4c/v7t+IwOAAClERTo0qCrftg08G9zNtlEJy+/0NG44BlIdAAAPiUk8HTNzppdRzR+0Tb7PNAVYI+TqFu9ksPRoaKR6AAAfPKoiOSdR+2jyNJth+xmg/AvfpvoUIwMAL7p5svq2RPOM4+ftOO9Gce1dNthHcrJczo0OIBiZIqRAcCnLdiUrkGTVqhtbDV99HAXp8NBBX//9ugNAwEAKK3gMzU7FCf7J79dugIA+IegwAD7687DxzRg4vJzXqtXvZJG922lkCB+7vdVfpvoUKMDAP6hTrUw++uxvAIt2nLgvNfNSehJjWs6EBkqAjU61OgAgM9bteOIdhzKOefay59vsUXL/x7U0R4SCu/ChoEAAJzRvkEN+zjblCU7bKKTX+DXP+/7PBYlAQB+KfhM7c7JAoqUfRkzOgAAv+7GSs/M1e4jx855LSw4UFFVQh2KDGWJRAcA4LdnZBnPfrzBPn7sb7e2Uf+O9R2IDGXJb5euTMdVQkKCOnbs6HQoAAAH3NA6RhGhQQoLdp3zCHKdXtL6dneG0yGiDNB1RdcVAOAs4xak6MXPNqt/hzj97ba2ToeDi2BnZAAASsCcdG7kF/r1PIDPINEBAOAsRUtXBYV0Y/kCEh0AAC6Q6DCj4xvougIA4CyBZ7qx0jJzL3hkhJFQtyrt516CRAcAgLOEnNlIcEXqkfMOAS0SUzVM34zoLteZ2R94Lr9NdDjUEwBwIebcq6ubRulQdt55rxWeOqVN+7O0PzPXLm2FkOh4PNrLaS8HABRT9ol8tR71mX2+6S+97A7KcAbt5QAAlFOhskGxsncg0QEAoJhcAT8kOgUkOl6BRAcAgEvcTNAoJNHxCiQ6AAAU09m1xwX+XeLqNUh0AAAopoCAAHeyw9KVd/Db9nIAAEoiyOVSXkGh3lm2U1XDfvrbaHCgS71axyi6aliFxYdzkegAAHAJKoUEKu94oV6bv7VY71+Relhv3HV5uceFC/PbRIcNAwEAJfHXm9vo8w37f/Z9u48c18odR3Tk2PkbD6LisGEgGwYCAMrBR2v36pH/rlZSo5r67wNXOB2Oz2HDQAAAHBR4Zs8durOcRaIDAEA5KOrO8vOFE8eR6AAAUE6t6AZd6M4i0QEAoBx3UWa/HWeR6AAAUA5YuvIMJDoAAJQD15lMhwkdZ5HoAABQjieds3TlLBIdAADKcemqkKUrR5HoAABQjvvokOc4y2+PgAAAoCLayzenZSl+xOxSfVbjWpX18bAuCg/h2/al8tsZHXPOVUJCgjp27Oh0KAAAH9Q0uoqqhweXyWd9fyBHKenZZfJZ/oazrjjrCgBQTk7kFygrN79Un3HTa19rf2auZg69Solx1cssNn/5/s0cGAAA5SQ0KFChVQLLZONBv56VKAW/XboCAMAbuM58p6Z7q2RIdAAA8GABonurNEh0AADwYGeat1i8KiESHQAAvGCHZTZYLhkSHQAAPFjRhA5LVyVDogMAgCfjKIlSIdEBAMALlq7Ic0qGRAcAAG9YuqIYuURIdAAA8GDM6JQOiQ4AAF7QXk6iUzIkOgAAeAGWrkqGRAcAAA/GPjqlQ6IDAIBXLF2R6fhtojNr1iw1b95cTZs21YQJE5wOBwCAMkMxcukEycvl5+dr+PDhWrBggapVq6b27dvr5ptvVs2aNZ0ODQCAspvRoUbHP2d0li9frlatWqlevXqqUqWKevfurblz5zodFgAAZYIjILw80Vm0aJH69OmjunXrKiAgQDNmzDjvPePGjVN8fLzCwsLUuXNnm9wU2bt3r01yipjne/bsqbD4AQAoT+Z7o0ExspcuXeXk5CgxMVH33XefbrnllvNenzZtml2aevPNN22S8+qrr6pnz57avHmzateufcl/3okTJ+yjSGZmZqn/DgAAlPfS1Sufb9HUpTvkjV66ra1qVw3zz0THLDWZx8W8/PLLGjx4sAYNGmTHJuGZPXu2Jk6cqBEjRtiZoLNncMzzTp06XfTzxowZo9GjR5fx3wIAgPJROyLU/rphX6a0T14p92ShY392wCkP6lcz03PTp09Xv3797DgvL0/h4eF6//333deMgQMH6ujRo5o5c6YtRm7ZsqUWLlzoLkb+5ptvLlqMfKEZnbi4OGVkZKhq1aoV8LcEAKD4juTkadHWAyrw4rWr61vFqEpo2c6tmO/f5vv+z33/dnxG56ccPHhQBQUFio6OPue6GW/atMk+DwoK0tixY9WtWzcVFhbqySef/MmOq9DQUPsAAMAb1Kgcol+2+6EWFZfGoxOd4urbt699AAAAeFTX1U+JiopSYGCg0tLSzrluxjExMaX6bNPJlZCQoI4dO5YySgAA4Kk8OtEJCQmxNTfz5893XzPLU2aclJRUqs8eOnSoNmzYoBUrVpRBpAAAwBM5vnSVnZ2tlJQU93j79u1as2aNIiMjVb9+fdtaboqPO3ToYLupTHu5aUkv6sICAADw2ERn5cqVtpC4iElsDJPcTJo0Sf3799eBAwc0cuRI7d+/X+3atdOcOXPOK1AuydKVeZhiZwAA4Js8qr3cCcVtTwMAAN73/duja3QAAABKg0QHAAD4LL9NdGgvBwDA91GjQ40OAABehxodAADg90h0AACAzyLRAQAAPsvxDQOdUrRhYH5+vnutDwAAeIei79s/V2rs98XIu3fvVlxcnNNhAACAEti1a5diY2Mv+rrfJzrmkNC9e/eqe/fu9jiKHzPt5xc6+PNC18++ZjJNk0CZ/wAV2c11sXjL8zOK8/6fes+lvlaca07cf2+89z/1ur/92y+Pe/9z7+Pel/wz+Lpz8bj85etOhw4d9MUXX6hu3bpyuS5eieO3S1dFzM0xmWBQUNAF/2EGBgYW+/qFrplxRX7BuVi85fkZxXn/T73nUl8r7rWKvv/eeO9/6nV/+7dfHvf+597HvS/5Z/B156dj8IevO0FBQT85k1OEYuQzhg4dWurrF3tvRSqLGC71M4rz/p96z6W+xr2/tPf/3Hv4t1+y31/c95fk376/3fuSfAZfd8ouhqE+/nXH75euygsbETqL++8c7r1zuPfO4v57JmZ0ykloaKhGjRplf0XF4/47h3vvHO69s7j/nokZHQAA4LOY0QEAAD6LRAcAAPgsEh0AAOCzSHQAAIDPItEBAAA+i0THIbNmzVLz5s3VtGlTTZgwwelw/MrNN9+sGjVq6LbbbnM6FL9itsXv2rWrEhIS1LZtW7333ntOh+RXjh49arfMb9eunVq3bq3x48c7HZLfOXbsmBo0aKAnnnjC6VD8Cu3lDjAnppsv9gsWLLCbS7Vv317ffPONatas6XRofmHhwoXKysrS5MmT9f777zsdjt/Yt2+f0tLS7Dfa/fv323/3W7ZsUeXKlZ0OzS8UFBToxIkTCg8PV05Ojk12zPl+fN2pOM8884xSUlLseVgvvfSS0+H4DWZ0HLB8+XK1atVK9erVU5UqVdS7d2/NnTvX6bD8hplViIiIcDoMv1OnTh2b5BgxMTGKiorS4cOHnQ7Lb5izgkySY5iEx/yMy8+5FWfr1q3atGmT/XqPikWiUwKLFi1Snz597ImpAQEBmjFjxnnvGTdunOLj4xUWFqbOnTvb5KaIOS3dJDlFzPM9e/ZUWPz+fO/hGfd+1apVdobB/GSLirv/ZvkqMTHRHoT4+9//3iabqJh7b5arxowZU4FRowiJTgmYaV/zxcL8w76QadOmafjw4XYr8OTkZPvenj17Kj09vcJj9TXce++/92YWZ8CAAXr77bcrKHLfUBb3v3r16lq7dq22b9+ud9991y4lovzv/cyZM9WsWTP7gANMjQ5KztzC6dOnn3OtU6dOp4YOHeoeFxQUnKpbt+6pMWPG2PHixYtP9evXz/36o48+euqdd96pwKj9994XWbBgwalbb721wmL1NSW997m5uaeuvvrqU1OmTKnQeH1Naf7tFxkyZMip9957r9xj9TUlufcjRow4FRsbe6pBgwanataseapq1aqnRo8eXeGx+ytmdMpYXl6enZbv0aOH+5rL5bLjJUuW2HGnTp20fv16u1yVnZ2tTz/91Gb/KP97D+fuvfkece+996p79+665557HIzWP++/mb0xRfiGOV3bLMeYzk+U/703S1am6zA1NdUWIQ8ePFgjR450MGr/EuR0AL7m4MGDtvYgOjr6nOtmbArRjKCgII0dO1bdunVTYWGhnnzySTofKujeG+YLkJm+N9PRplbBtDknJSU5ELF/3fvFixfbKX7TWl5U4zB16lS1adPGkZj97f7v2LFDDzzwgLsIediwYdz7Cvy6A+eQ6Dikb9++9oGKN2/ePKdD8EtdunSxiT2cYWaS16xZ43QYfs/MaqJisXRVxkwXg2nj/HGRnxmbllqUH+69c7j3zuL+O4d77/lIdMpYSEiI3Qht/vz57mvmp1gzZnmkfHHvncO9dxb33znce8/H0lUJmAJis7tlEdOqaaaEIyMjVb9+fdtmOHDgQLvdupkufvXVV209yKBBgxyN2xdw753DvXcW99853Hsv53Tblzcyrcnm1v34MXDgQPd7Xn/99VP169c/FRISYlsPly5d6mjMvoJ77xzuvbO4/87h3ns3zroCAAA+ixodAADgs0h0AACAzyLRAQAAPotEBwAA+CwSHQAA4LNIdAAAgM8i0QEAAD6LRAcAAPgsEh0Afn2SdL9+/ZwOA0A5ItEBAAA+i0QHgNfJy8tzOgQAXoJEB4DH69q1qx5++GH97ne/U1RUlHr27KmXX35Zbdq0UeXKlRUXF6eHHnrInjJdZNKkSapevbo+++wztWzZUlWqVFGvXr20b9++i/45K1asUK1atfS3v/2tgv5mAMobiQ4ArzB58mSFhIRo8eLFevPNN+VyufTaa6/pu+++s6998cUXevLJJ8/5PceOHdNLL72kqVOnatGiRdq5c6eeeOKJC36++f3XXXednn/+eT311FMV9LcCUN6Cyv1PAIAy0LRpU73wwgvucfPmzd3P4+Pj9dxzz+nBBx/UP/7xD/f1kydP2qSocePGdmxmhf785z+f99nTp0/XgAEDNGHCBPXv37/c/y4AKg6JDgCv0L59+3PG8+bN05gxY7Rp0yZlZmYqPz9fubm5dhYnPDzcvsf8WpTkGHXq1FF6evo5n7Ns2TLNmjVL77//Ph1YgA9i6QqAVzC1OEVSU1N10003qW3btvrggw+0atUqjRs37rxC5eDg4HM+IyAgQKdOnTrnmkmEWrRooYkTJ9oZIAC+hUQHgNcxiU1hYaHGjh2rK664Qs2aNdPevXtL9FmmuNnU56SkpOiOO+4g2QF8DIkOAK/TpEkTm5C8/vrr2rZtmy02NrU4JVW7dm2b7JhlsDvvvNMugwHwDSQ6ALxOYmKibS83beCtW7fWO++8Y+t1SiMmJsYmO+vWrdPdd9+tgoKCMosXgHMCTv14wRoAAMBHMKMDAAB8FokOAADwWSQ6AADAZ5HoAAAAn0WiAwAAfBaJDgAA8FkkOgAAwGeR6AAAAJ9FogMAAHwWiQ4AAPBZJDoAAMBnkegAAAD5qv8PnUgh774CPJMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plot shows that the distribution of the words in the vocabulary follows the Zipf's law. The most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n",
    "\n",
    "We can also filter the vocabulary by the frequency of the words. We will only consider the most frequent words and mark the rest as the `<unk>` token. Here we set the maximum vocabulary size to 10,000. But in the later steps, you will experiment with different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.419787Z",
     "start_time": "2025-04-10T15:14:14.618624Z"
    }
   },
   "source": [
    "max_vocab_size = 10000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "print(len(vocab))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 4: Frequency of pairs of words (bigrams)\n",
    "Calculate the frequency of (neighbouring) pairs of words in the training dataset.\n",
    "- (5p) List the most and least common pairs. Do the most common pairs make sense?\n",
    "\n",
    "    10 most common pairs: (('@', 'user'), 12236), (('.', '.'), 6603), (('!', '!'), 4401), ((',', 'california'), 3343), (('️', '@'), 3287), (('.', '#'), 2784), (('!', '#'), 2623), (('user', '@'), 2443), ((\"'\", 's'), 2424), (('️', '️'), 2359)\n",
    "\n",
    "    10 least common pairs: (('through', 'venice'), 1), (('some', 'bbq'), 1), (('and', 'whiskey'), 1), (('whiskey', 'libations'), 1), (('libations', '.'), 1), (('.', 'chomp'), 1), (('chomp', ','), 1), ((',', 'belch'), 1), (('belch', ','), 1), ((',', 'chomp'), 1)\n",
    "\n",
    "- (2p) How many pairs occur only once in the dataset? 187683\n",
    "- (5p) Plot the distribution of the pair frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.421271Z",
     "start_time": "2025-04-11T08:08:35.786990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('.', '.'), 3700), ((',', 'california'), 3343), (('.', '#'), 3175), (('!', '#'), 3021), (('!', '!'), 2972), ((\"'\", 's'), 2419), (('&', 'amp'), 2134), (('amp', ';'), 2134), (('.', '@'), 1897), (('!', '@'), 1762)]\n",
      "[(('through', 'venice'), 1), (('some', 'bbq'), 1), (('and', 'whiskey'), 1), (('whiskey', 'libations'), 1), (('libations', '.'), 1), (('.', 'chomp'), 1), (('chomp', ','), 1), ((',', 'belch'), 1), (('belch', ','), 1), ((',', 'chomp'), 1)]\n"
     ]
    }
   ],
   "source": [
    "def pairs(data):\n",
    "    pair_counts = Counter()\n",
    "\n",
    "    for tweet in data:\n",
    "        text = tweet['clean']\n",
    "        tokens = text.split()\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            pair_counts[pair] += 1\n",
    "\n",
    "    return pair_counts\n",
    "\n",
    "def least_common(counter, n=10):\n",
    "    return sorted(counter.items(), key=lambda x: x[1])[:n]\n",
    "\n",
    "bigram = pairs(tweet_ds['train'])\n",
    "print(bigram.most_common(10))\n",
    "print(least_common(bigram))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sorted_counts = sorted(bigram.items(), key=lambda x: x[1], reverse=True)\n",
    "bigram_df = pd.DataFrame(sorted_counts, columns=['pair', 'count'])\n",
    "\n",
    "bigram2_df = bigram_df.sort_values(by='count', ascending=False).copy()\n",
    "bigram2_df['rank'] = range(1, len(bigram2_df) + 1)\n",
    "\n",
    "bigram2_df.plot(x='rank', y='count', kind='line', figsize=(12, 6), logy=True, logx=True)\n",
    "plt.title(\"Bigrams Frequency Distribution (log-log scale)\")\n",
    "plt.xlabel(\"Rank (log)\")\n",
    "plt.ylabel(\"Frequency (log)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "bigram2_df = bigram_df[bigram_df['count'] > 100].copy() # gets rid of words with fewer then or equal to occurances\n",
    "bigram2_df['rank'] = range(1, len(bigram2_df) + 1)\n",
    "\n",
    "bigram2_df.plot(x='rank', y='count', kind='line', figsize=(12, 6), logy=True)\n",
    "plt.title(\"Filtered Bigram Frequency Distribution (log scale)\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Log Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.3 Tokenize the dataset\n",
    "The function below tokenizes the cleaned text (```example['clean']```) by splitting it on spaces. It replaces the words that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 5: Tokenize the dataset\n",
    "\n",
    "(5p) Fill in the function below to tokenize the dataset. The function will be applied to the dataset through the `map()` method, so it returns the updated example. Your task is to split the text by spaces and replace the words that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.424256Z",
     "start_time": "2025-04-10T15:14:17.618112Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(example, vocab, unknown_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a vocabulary as a list of words\n",
    "        unknown_token: a token to replace the words that are not in the vocabulary\n",
    "    Returns: update example containing 'tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    tokens = None # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [token if token in vocab else unknown_token for token in tokens]\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    example['tokens'] = tokens\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.424328Z",
     "start_time": "2025-04-10T15:14:17.627009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45000/45000 [00:08<00:00, 5232.39 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:10<00:00, 4584.93 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4975.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let us examine several entries from the dataset. We can see that the `tokens` column has been added to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.425748Z",
     "start_time": "2025-04-10T15:14:38.722139Z"
    }
   },
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['train'][i]['text'])\n",
    "    print('Tokenized tweet:')\n",
    "    print(tweet_ds['train'][i]['tokens'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "Sunday afternoon walking through Venice in the sun with @user ️ ️ ️ @ Abbot Kinney, Venice\n",
      "Tokenized tweet:\n",
      "['sunday', 'afternoon', 'walking', 'through', 'venice', 'in', 'the', 'sun', 'with', 'abbot', 'kinney', ',', 'venice']\n",
      "Original tweet:\n",
      "Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que)\n",
      "Tokenized tweet:\n",
      "['time', 'for', 'some', 'bbq', 'and', 'whiskey', 'libations', '.', '<unk>', ',', '<unk>', ',', '<unk>', '!', '(', 'lucille', \"'\", 's', 'smokehouse', 'bar', '-', 'b', '-', 'que', ')']\n",
      "Original tweet:\n",
      "Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San…\n",
      "Tokenized tweet:\n",
      "['love', 'love', 'love', 'all', 'these', 'people', 'friends', 'bff', 'celebrate', 'blessed', 'sundayfunday', 'san', '…']\n",
      "Original tweet:\n",
      "️ ️ ️ ️ @ Toys\"R\"Us\n",
      "Tokenized tweet:\n",
      "['toys', '\"', 'r', '\"', 'us']\n",
      "Original tweet:\n",
      "Man these are the funniest kids ever!! That face! #HappyBirthdayBubb @ FLIPnOUT Xtreme\n",
      "Tokenized tweet:\n",
      "['man', 'these', 'are', 'the', 'funniest', 'kids', 'ever', '!', '!', 'that', 'face', '!', '<unk>', '<unk>', '<unk>']\n",
      "Original tweet:\n",
      "#sandiego @ San Diego, California\n",
      "Tokenized tweet:\n",
      "['sandiego', 'san', 'diego', ',', 'california']\n",
      "Original tweet:\n",
      "My little ️ ️ ️ ️ ️ #ObsessedWithMyDog @ Cafe Solstice Capitol Hill\n",
      "Tokenized tweet:\n",
      "['my', 'little', '<unk>', 'cafe', 'solstice', 'capitol', 'hill']\n",
      "Original tweet:\n",
      "More #tinyepic things #tinyepicwestern, this one is crazy @user I may be one of your…\n",
      "Tokenized tweet:\n",
      "['more', '<unk>', 'things', '<unk>', ',', 'this', 'one', 'is', 'crazy', 'i', 'may', 'be', 'one', 'of', 'your', '…']\n",
      "Original tweet:\n",
      "Last night ️ @ Omnia Night Club At Caesars Palace\n",
      "Tokenized tweet:\n",
      "['last', 'night', 'omnia', 'night', 'club', 'at', 'caesars', 'palace']\n",
      "Original tweet:\n",
      "friendship at its finest. ....#pixar #toystory #buzz #woody #friends #friendship #bff…\n",
      "Tokenized tweet:\n",
      "['friendship', 'at', 'its', 'finest', '.', '.', '.', '<unk>', '<unk>', 'buzz', 'woody', 'friends', 'friendship', 'bff', '…']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Make sure that the tokenization works as you intended. If not, revisit the cleaning and tokenization functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 6: Questions about the tokenization\n",
    "1. (3p) How many unknown tokens are in the validation dataset after tokenization? 7595\n",
    "2. (3p) What is the distribution of the number of tokens in the training dataset?\n",
    "    mMst tweets are composed of around 10 to 20 token, with tweets of less than 5 tokens and more than  30 being rarer\n",
    "\n",
    "3. (4p) How the number of tokens corresponds to the number of characters in our dataset?\n",
    "    Total tokens:  665951, and\n",
    "    total chars:  3195761.\n",
    "    There are ≈4.8 times more characters than tokens, so roughly 4-5 characters per token if we look purely at the numbers.\n",
    "4. (4p) How the size of the vocabulary (```max_vocab_size```) affects the number of unknown tokens?\n",
    "    As expected the larger the max_vocab_size the fewer the unknown tokens, conversly the smaller it is the greater the number of unk tokens, this is clearly represented in the graph below\n",
    "5. (4p) How does the size of the vocabulary affect the number of tokens in the dataset?\n",
    "    Regardless of vocab size the total number of tokens doesn't change, just the number of unk tokens changes and obviously the number of unique tokens.\n",
    "6. (4p) Think about the advantages and disadvantages of the tokenization method we used. What are the cases when it will not work well?\n",
    "    Advantages:\n",
    "   A. easy and efficient to implement\n",
    "   B. the tokens are easy to understand and interpret as they resemble natural language\n",
    "    Disadvantages:\n",
    "   A. doesn't treat words with the same subword (stem) as a unit thereby introducing more tokens than necessary, ie. walking and walked.\n",
    "   B. doesn't lend itself well to texts with a lot of irregularities in expression, ie. informal language, abbreviations, slang, emojis, weird punctuation etc.\n",
    "   C. doesn't handle misspellings, or different spellings of the same word well, and will treat those as different tokens\n",
    "\n",
    "The tokenizer will work well for texts written in standard, grammatically correct language, where spelling, punctuation, and word choice are consistent. It performs best when the variety of word types is small and language use is relatively formal or controlled.\n",
    "Whilst for texts with many means of expression, be it through; emoji's, punctuation, slang, or any unconventional or creative use of spelling/writing will render this tokeniser ineffective, as this will result in a sparse vocabulary, and generally many unk tokens, losing in generality and nuance.\n",
    "\n",
    "\n",
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.427261Z",
     "start_time": "2025-04-11T08:09:21.614139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7472\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for tweet in tweet_ds['validation']:\n",
    "    text = tweet['tokens']\n",
    "    for token in text:\n",
    "        if token == '<unk>':\n",
    "            count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "tokens_per_tweet = [len(tweet['tokens']) for tweet in tweet_ds['train']]\n",
    "\n",
    "plt.hist(tokens_per_tweet, bins=39) # bins trickery if you change bins to other numbers like 20 or 30 it will look very weird whilst if you wnet with a bit higher there would be empty bars\n",
    "plt.title(\"Distribution of Token Counts per Tweet\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.430145Z",
     "start_time": "2025-04-10T15:14:41.299988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statistics import mean\n",
    "\n",
    "total_tokens = 0\n",
    "for tweet in tweet_ds['train']:\n",
    "    total_tokens += len(tweet['tokens'])\n",
    "\n",
    "total_characters = 0\n",
    "for tweet in tweet_ds['train']:\n",
    "    total_characters += len(tweet['text'])\n",
    "\n",
    "print('total tokens: ', total_tokens)\n",
    "print('total chars: ', total_characters)\n",
    "print('av ', total_characters/total_tokens)\n",
    "\n",
    "plt.scatter([len(tokens) for tokens in tweet_ds['train']['tokens']], [len(example['clean']) for example in tweet_ds['train']])\n",
    "plt.title(\"Number of token against Characters per Tweet\")\n",
    "plt.xlabel('Number of tokens')\n",
    "plt.ylabel('Number of characters')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mean_token_length_per_tweet = [mean([len(token) for token in tweet['tokens']]) for tweet in tweet_ds['train']]\n",
    "plt.hist(mean_token_length_per_tweet, bins=40)\n",
    "plt.title(\"Distribution of Token Length\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Token Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens:  665951\n",
      "total chars:  3195761\n",
      "av  4.798793004290105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 396
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = []\n",
    "\n",
    "max_vocab_sizes = [100, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "\n",
    "for max_vocab_size in max_vocab_sizes:\n",
    "    vocab = vocab_counter.most_common(max_vocab_size)\n",
    "    vocab = [word for word, _ in vocab]\n",
    "\n",
    "    unknown_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for tweet in tweet_ds['train']:\n",
    "        text = tweet['clean']\n",
    "        total_count += len(text)\n",
    "        for token in text.split():\n",
    "            if token not in vocab:\n",
    "                unknown_count += 1\n",
    "\n",
    "    results.append({\n",
    "        'vocab_size': max_vocab_size,\n",
    "        'unknown_tokens': unknown_count,\n",
    "        'total_tokens': total_count,\n",
    "        'percentage_unknown': round(unknown_count / total_count * 100, 2)\n",
    "    })\n",
    "\n",
    "df_unknowns = pd.DataFrame(results)\n",
    "\n",
    "# probably a log plot may be more appropriate but I thought that this illustrates the relationship more intuitively\n",
    "df_unknowns.plot(x='vocab_size', y='unknown_tokens', kind='line', marker='o', figsize=(10, 5))\n",
    "plt.title(\"Effect of Vocab Size on Number of Unknown Tokens\")\n",
    "plt.xlabel(\"Max Vocabulary Size\")\n",
    "plt.ylabel(\"Unknown Tokens\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Byte Pair Encoding\n",
    "\n",
    "In this section, you will build the Byte Pair Encoding (BPE) tokenizer. BPE is an algorithm that replaces the most frequent pair of tokens (initially characters) with a new token. The algorithm is configured by the number of merges that are performed. You can find the paper here [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Finding the initial set of characters\n",
    "BPE algorithm starts with the set of characters that occur in the dataset. We will build a character counter from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 7: Counting the characters\n",
    "\n",
    "(5p) In this exercise, we build a counter with the frequencies of all characters in the dataset. Iterate over the dataset and count the characters in the `clean` column. The function returns a `Counter` object with the characters and their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.433052Z",
     "start_time": "2025-04-10T15:15:37.140436Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_character_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a character counter from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a character counter\n",
    "\n",
    "    \"\"\"\n",
    "    char_counter = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    for tweet in dataset:\n",
    "        text = tweet['clean']\n",
    "        char_counter.update(text)\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return char_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next cell applies the function to the training dataset and prints the size of the vocabulary and the most common characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.434530Z",
     "start_time": "2025-04-10T15:15:37.340277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508\n",
      "[(' ', 620951), ('e', 250924), ('a', 223897), ('o', 192520), ('t', 186996), ('i', 178537), ('s', 159612), ('n', 157625), ('r', 139661), ('l', 122189), ('h', 106487), ('d', 80949), ('m', 73633), ('c', 73512), ('y', 70346), ('u', 67823), ('g', 62868), ('#', 56729), ('f', 54106), ('p', 51365), ('w', 47337), ('b', 46168), ('k', 31133), ('v', 30783), ('@', 24209), ('.', 19639), ('…', 19238), ('!', 14981), (',', 12382), ('j', 6632), ('x', 4439), ('1', 4435), ('z', 4336), ('2', 3578), ('0', 3528), ('-', 3287), (\"'\", 3236), (':', 2782), ('\"', 2362), (';', 2354), ('(', 2297), ('&', 2243), (')', 2089), ('q', 2075), ('6', 1847), ('/', 1698), ('?', 1551), ('5', 1550), ('_', 1458), ('3', 1423), ('4', 1092), ('7', 934), ('|', 906), ('9', 815), ('8', 759), ('•', 447), ('・', 428), ('+', 282), ('*', 245), ('$', 177), ('—', 129), ('~', 128), ('[', 127), ('é', 126), ('%', 126), (']', 118), ('=', 106), ('{', 63), ('}', 53), ('ñ', 44), ('♡', 30), ('о', 25), ('и', 23), ('⠀', 21), ('͟', 18), ('р', 17), ('\\\\', 17), ('^', 17), ('ン', 15), ('н', 12), ('ا', 12), ('а', 11), ('б', 10), ('ア', 10), ('¡', 10), ('ᴵ', 10), ('ˢ', 10), ('á', 9), ('︎', 9), ('í', 9), ('к', 8), ('ー', 8), ('è', 8), ('φ', 8), ('ᴬ', 8), ('ᴱ', 8), ('α', 8), ('ス', 7), ('–', 7), ('σ', 7)]\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will filter the characters that occur less than 10 times in the dataset. We will also replace the space character with the `__` token. This is necessary because we want to preserve the spaces between the words in the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.434632Z",
     "start_time": "2025-04-10T15:15:39.207779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', 'é', 'ñ', 'ˢ', '͟', 'а', 'б', 'и', 'н', 'о', 'р', 'ا', 'ᴵ', '—', '•', '…', '♡', '⠀', 'ア', 'ン', '・']\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Training the BPE tokenizer\n",
    "\n",
    "In this section, we will implement the BPE algorithm. We will start by initializing the BPE corpus. The corpus is a list of words from the dataset with their frequency. This makes it easier to find the most frequent pairs of neighbouring tokens (or characters in the beginning). Each word is split into characters and the space (the ```__``` token) is added at the end of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.434710Z",
     "start_time": "2025-04-10T15:15:39.405828Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_bpe_corpus(dataset):\n",
    "    \"\"\"\n",
    "    Initializes the BPE corpus\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a BPE corpus\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = Counter()\n",
    "    for example in dataset:\n",
    "        words = example['clean'].split()\n",
    "        words = [' '.join(list(word)) + ' __' for word in words]\n",
    "        corpus.update(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.436357Z",
     "start_time": "2025-04-10T15:15:39.614415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53636\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the most common words in the corpus along with their frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.436494Z",
     "start_time": "2025-04-10T15:15:41.586112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# __', 56729),\n",
       " ('@ __', 24209),\n",
       " ('. __', 19639),\n",
       " ('… __', 19238),\n",
       " ('! __', 14981),\n",
       " ('t h e __', 13881),\n",
       " (', __', 12382),\n",
       " ('i __', 8557),\n",
       " ('t o __', 7843),\n",
       " ('m y __', 7657),\n",
       " ('a __', 7025),\n",
       " ('i n __', 6105),\n",
       " ('y o u __', 5890),\n",
       " ('i s __', 5828),\n",
       " ('a n d __', 5799),\n",
       " ('w i t h __', 5278),\n",
       " ('o f __', 5194),\n",
       " ('f o r __', 4876),\n",
       " ('t h i s __', 4626),\n",
       " ('c a l i f o r n i a __', 4500),\n",
       " ('a t __', 3739),\n",
       " ('i t __', 3615),\n",
       " ('l o v e __', 3577),\n",
       " ('- __', 3287),\n",
       " (\"' __\", 3236),\n",
       " ('o n __', 2844),\n",
       " (': __', 2782),\n",
       " ('s __', 2674),\n",
       " ('n o t __', 2568),\n",
       " ('\" __', 2362)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our BPE implementation will consist of the following steps:\n",
    "1. Calculate the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "2. Find the most frequent pair.\n",
    "3. Merge the most frequent pair.\n",
    "4. Repeat until the specified number of merges is reached.\n",
    "\n",
    "The following function calculates the frequency statistics of adjacent symbol pairs in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.436636Z",
     "start_time": "2025-04-11T07:53:11.917496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 8: Calculate the frequency statistics of adjacent symbol pairs\n",
    "(5p) Fill in the function below to calculate the frequency statistics of adjacent symbol pairs in the corpus. The function returns a Counter object with the counts of adjacent token pairs. The pairs are represented as tuples of two tokens (e.g., `('cali', 'for')`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.436708Z",
     "start_time": "2025-04-10T15:15:41.994824Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bpe_corpus_stats(corpus):\n",
    "    \"\"\"\n",
    "    Calculates the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "    Args:\n",
    "        corpus: a BPE corpus as a Counter object with words split by space into tokens (initially characters)\n",
    "\n",
    "    Returns: a Counter object with the frequency statistics of adjacent symbol pairs\n",
    "    \"\"\"\n",
    "    stats = Counter()\n",
    "\n",
    "    for word, freq in corpus.items():\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        chars = word.split()\n",
    "        for i in range(len(chars) - 1):\n",
    "            pair = (chars[i], chars[i + 1])\n",
    "            stats[pair] += freq\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the most common pairs of characters in the initial corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.438191Z",
     "start_time": "2025-04-10T15:15:42.194458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('e', '__'), 79895), (('s', '__'), 60251), (('#', '__'), 56729), (('t', '__'), 47089), (('t', 'h'), 42868), (('y', '__'), 41976), (('i', 'n'), 40342), (('a', 'n'), 35089), (('n', '__'), 34154), (('h', 'e'), 30818)]\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we will implement the function that merges the most frequent pair of symbols in the corpus. The function takes the corpus and the most frequent pair of symbols as input and returns the updated corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.438303Z",
     "start_time": "2025-04-10T15:15:42.510078Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_corpus(corpus, pair):\n",
    "    \"\"\"\n",
    "    Merges the most frequent pair of symbols in the corpus.\n",
    "    Args:\n",
    "        corpus (dict): Keys are words as space-separated symbols (e.g., \"l o w\"),\n",
    "                       and values are the frequency counts.\n",
    "        pair (tuple): A pair of symbols to merge.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated corpus after merging the pair of symbols.\n",
    "    \"\"\"\n",
    "    new_corpus = Counter()\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    for word, freq in corpus.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_corpus[new_word] = freq\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The last step is to implement the BPE algorithm. The function takes the initial vocabulary, the corpus, and the number of merges as input. It returns the updated vocabulary, corpus, and the list of merges.\n",
    "Returning the list of merges is useful for the tokenization process - it makes it faster to tokenize the text. It contains the tuples of the two tokens that were merged. For example, ('to', 'day__') will merge the tokens 'to' and 'day__' into the 'today__' token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 9: BPE algorithm\n",
    "\n",
    "(10p) Implement the BPE algorithm in the following function. The function should return the updated vocabulary, corpus, and the list of merges. The function should perform the specified number of merges. The vocabulary is a list of tokens, the corpus is a Counter object with the words split by space into tokens, and the merges is a list of tuples with the merged tokens.\n",
    "\n",
    "You should use the functions you implemented earlier in this section (```calculate_bpe_corpus_stats()```, ```merge_corpus()```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.439747Z",
     "start_time": "2025-04-10T15:15:42.703545Z"
    }
   },
   "outputs": [],
   "source": [
    "def bpe(vocab, corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Applies the BPE algorithm to the corpus. Merges the most frequent adjacent symbol pairs. The function performs the specified number of merges.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): A list of tokens representing the BPE vocabulary.\n",
    "        corpus (Counter): A Counter object with words split by space into tokens.\n",
    "        num_merges (int): The number of merges to perform.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated vocabulary.\n",
    "        Counter: Updated corpus.\n",
    "        list: List of merges.\n",
    "    \"\"\"\n",
    "    vocab = vocab.copy()\n",
    "    corpus = corpus.copy()\n",
    "    merges = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_merges)):\n",
    "        ### YOUR CODE HERE\n",
    "        stats = calculate_bpe_corpus_stats(corpus)\n",
    "        most_common = stats.most_common(1)[0][0]\n",
    "        merges.append(most_common)\n",
    "        corpus = merge_corpus(corpus, most_common)\n",
    "        merged_string = ''.join(most_common)\n",
    "        vocab.append(merged_string)\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return vocab, corpus, merges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following cell applies the BPE algorithm to the initial vocabulary and corpus. We will perform 100 merges at first, but you will experiment with different numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.441170Z",
     "start_time": "2025-04-10T15:15:42.899493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.23it/s]\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can check the size of the BPE vocabulary and the most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.441229Z",
     "start_time": "2025-04-10T15:15:54.437502Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n",
      "['__', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', 'é', 'ñ', 'ˢ', '͟', 'а', 'б', 'и', 'н', 'о', 'р', 'ا', 'ᴵ', '—', '•', '…', '♡', '⠀', 'ア', 'ン', '・', 'e__', 's__', '#__', 't__', 'th', 'y__', 'in', 'an', 'er', 'd__', 'a__', 'or', '@__', 'o__', 'on', 'al', 'ou', 'ar', '.__', '…__', 'ing', 'en', 'st', '!__', 'ing__', 'the__', 'ch', 're', 'l__', 'lo', ',__', 'am', 'er__', 'is__', 'at', 'i__', 'for', 'da', 'om', 'el', 'ri', 'k__', 'on__', 'be', 'li', 'to__', 'ni', 'es__', 'in__', 'la', 'ho', 'you', 'f__', 'my__', 'wi', 've__', 'p__', 'day__', 'ha', 'w__', 'th__', 'ti', 'oo', 'ne', 'and__', 'di', 'gh', 'as__', 'an__', 'se', 'you__', 'un', 'le', 'ir', 'ac', 'with__', 'no', 'of__', 'it__', 'ro', 'cali', 'for__', 'si', 'sh', 'califor', 'to', 'californi', 'this__', 'pp', 'california__', 'il', 'we', 'me__', 'en__', 'the', 'r__', 've', 'ra', 'ma', 'love__']\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also check the most common merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.441291Z",
     "start_time": "2025-04-10T15:15:54.645965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e', '__'), ('s', '__'), ('#', '__'), ('t', '__'), ('t', 'h'), ('y', '__'), ('i', 'n'), ('a', 'n'), ('e', 'r'), ('d', '__'), ('a', '__'), ('o', 'r'), ('@', '__'), ('o', '__'), ('o', 'n'), ('a', 'l'), ('o', 'u'), ('a', 'r'), ('.', '__'), ('…', '__'), ('in', 'g'), ('e', 'n'), ('s', 't'), ('!', '__'), ('ing', '__'), ('th', 'e__'), ('c', 'h'), ('r', 'e'), ('l', '__'), ('l', 'o'), (',', '__'), ('a', 'm'), ('er', '__'), ('i', 's__'), ('a', 't'), ('i', '__'), ('f', 'or'), ('d', 'a'), ('o', 'm'), ('e', 'l'), ('r', 'i'), ('k', '__'), ('on', '__'), ('b', 'e'), ('l', 'i'), ('t', 'o__'), ('n', 'i'), ('e', 's__'), ('in', '__'), ('l', 'a'), ('h', 'o'), ('y', 'ou'), ('f', '__'), ('m', 'y__'), ('w', 'i'), ('v', 'e__'), ('p', '__'), ('da', 'y__'), ('h', 'a'), ('w', '__'), ('th', '__'), ('t', 'i'), ('o', 'o'), ('n', 'e'), ('an', 'd__'), ('d', 'i'), ('g', 'h'), ('a', 's__'), ('an', '__'), ('s', 'e'), ('you', '__'), ('u', 'n'), ('l', 'e'), ('i', 'r'), ('a', 'c'), ('wi', 'th__'), ('n', 'o'), ('o', 'f__'), ('i', 't__'), ('r', 'o'), ('c', 'ali'), ('for', '__'), ('s', 'i'), ('s', 'h'), ('cali', 'for'), ('t', 'o'), ('califor', 'ni'), ('th', 'is__'), ('p', 'p'), ('californi', 'a__'), ('i', 'l'), ('w', 'e'), ('m', 'e__'), ('en', '__'), ('th', 'e'), ('r', '__'), ('v', 'e'), ('r', 'a'), ('m', 'a'), ('lo', 've__')]\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 Tokenizing the text using BPE\n",
    "\n",
    "With the tokenizer trained we can now tokenize the text using the BPE vocabulary. We will first build a function that tokenizes any text using our BPE tokenizer (vocabulary and merges). Next we will apply it to our dataset.\n",
    "\n",
    "The following function tokenizes the text using the BPE vocabulary. It replaces the most frequent pairs of tokens with the new token. The function also replaces the tokens that are not in the vocabulary with the `<unk>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.441355Z",
     "start_time": "2025-04-10T15:15:54.850799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'un', 'day__', 'a', 'f', 't', 'er', 'no', 'on__', 'w', 'al', 'k', 'ing__', 'th', 'r', 'ou', 'gh', '__', 'v', 'en', 'i', 'c', 'e__', 'in__', 'the__', 's', 'un', '__', 'with__', '@__', 'a', 'b', 'b', 'o', 't__', 'k', 'in', 'ne', 'y__', ',__', 'v', 'en', 'i', 'c', 'e__', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "def apply_bpe_tokenization(text, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using BPE vocabulary, preserving spaces as '__'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        vocab (set): A set containing the BPE vocabulary tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens representing the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    words = re.split(r'\\s', text)\n",
    "    words = [' ' + ' '.join(list(word)) + (' __ ' if i < len(words) - 1 else ' ') for i, word in enumerate(words)]\n",
    "\n",
    "    bpe_tokens = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for merge in merges:\n",
    "            word = word.replace(' ' + ' '.join(merge) + ' ', ' ' + ''.join(merge) + ' ')\n",
    "        bpe_tokens.extend(word.split())\n",
    "\n",
    "    for i, token in enumerate(bpe_tokens):\n",
    "        if token not in vocab:\n",
    "            bpe_tokens[i] = unk_token\n",
    "    return bpe_tokens\n",
    "\n",
    "\n",
    "# A test example with a special character. Is the character tokenized correctly as <unk> token?\n",
    "print(apply_bpe_tokenization(tweet_ds['train'][0]['clean'] + ' 🇺', bpe_vocab, bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function below will apply our BPE tokenizer to the dataset. It will add a new column `bpe_tokens` to each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.442761Z",
     "start_time": "2025-04-10T15:15:55.061010Z"
    }
   },
   "source": [
    "def tokenize_bpe(example, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset using BPE\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a BPE vocabulary\n",
    "\n",
    "    Returns: update example containing 'bpe_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    bpe_tokens = apply_bpe_tokenization(text, vocab, merges, unk_token)\n",
    "    example['bpe_tokens'] = bpe_tokens\n",
    "    return example\n",
    "\n",
    "tweet_ds = tweet_ds.map(tokenize_bpe, fn_kwargs={'vocab': bpe_vocab, 'merges': bpe_merges})\n",
    "print(tweet_ds)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45000/45000 [00:15<00:00, 2841.73 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [00:18<00:00, 2735.55 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3122.97 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 45000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label', 'text_length', 'clean', 'tokens', 'bpe_tokens'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will inspect the both tokenizations of several examples from the ```validation``` subset. Try to find the ```<unk>``` tokens in the printed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:39:55.442837Z",
     "start_time": "2025-04-10T15:16:39.354811Z"
    }
   },
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['validation'][i]['text'])\n",
    "    print('Word tokenization:')\n",
    "    print(tweet_ds['validation'][i]['tokens'])\n",
    "    print('BPE tokenization:')\n",
    "    print(tweet_ds['validation'][i]['bpe_tokens'])\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:\n",
      "A little throwback with my favourite person @ Water Wall\n",
      "Word tokenization:\n",
      "['a', 'little', 'throwback', 'with', 'my', 'favourite', 'person', '@', 'water', 'wall']\n",
      "BPE tokenization:\n",
      "['a__', 'li', 't', 't', 'l', 'e__', 'th', 'ro', 'w', 'b', 'ac', 'k__', 'with__', 'my__', 'f', 'a', 'v', 'ou', 'ri', 't', 'e__', 'p', 'er', 's', 'on__', '@__', 'w', 'at', 'er__', 'w', 'al', 'l']\n",
      "\n",
      "Original tweet:\n",
      "glam on @user yesterday for #kcon makeup using @user in #featherette,…\n",
      "Word tokenization:\n",
      "['glam', 'on', 'yesterday', 'for', '#', 'kcon', 'makeup', 'using', 'in', '#', '<unk>', ',', '…']\n",
      "BPE tokenization:\n",
      "['g', 'l', 'am', '__', 'on__', 'y', 'e', 'st', 'er', 'day__', 'for__', '#__', 'k', 'c', 'on__', 'ma', 'k', 'e', 'u', 'p__', 'u', 's', 'ing__', 'in__', '#__', 'f', 'e', 'a', 'th', 'er', 'e', 't', 't', 'e__', ',__', '…']\n",
      "\n",
      "Original tweet:\n",
      "Democracy Plaza in the wake of a stunning outcome #Decision2016 @ NBC News\n",
      "Word tokenization:\n",
      "['<unk>', 'plaza', 'in', 'the', 'wake', 'of', 'a', 'stunning', '<unk>', '#', '<unk>', '@', '<unk>', 'news']\n",
      "BPE tokenization:\n",
      "['d', 'e', 'm', 'o', 'c', 'r', 'ac', 'y__', 'p', 'la', 'z', 'a__', 'in__', 'the__', 'w', 'a', 'k', 'e__', 'of__', 'a__', 'st', 'un', 'n', 'ing__', 'ou', 't', 'c', 'om', 'e__', '#__', 'd', 'e', 'c', 'i', 'si', 'on', '2', '0', '1', '6', '__', '@__', 'n', 'b', 'c', '__', 'ne', 'w', 's']\n",
      "\n",
      "Original tweet:\n",
      "Then &amp; Now. VILO @ Walt Disney Magic Kingdom\n",
      "Word tokenization:\n",
      "['then', '&', 'amp', ';', 'now', '.', '<unk>', '@', 'walt', 'disney', 'magic', 'kingdom']\n",
      "BPE tokenization:\n",
      "['th', 'en__', '&', '__', 'am', 'p__', ';', '__', 'no', 'w__', '.__', 'v', 'il', 'o__', '@__', 'w', 'al', 't__', 'di', 's', 'ne', 'y__', 'ma', 'g', 'i', 'c', '__', 'k', 'ing', 'd', 'om']\n",
      "\n",
      "Original tweet:\n",
      "Who never... @ A Galaxy Far Far Away\n",
      "Word tokenization:\n",
      "['who', 'never', '.', '.', '@', 'a', 'galaxy', 'far', 'far', 'away']\n",
      "BPE tokenization:\n",
      "['w', 'h', 'o__', 'ne', 'v', 'er__', '.__', '.__', '@__', 'a__', 'g', 'al', 'a', 'x', 'y__', 'f', 'ar', '__', 'f', 'ar', '__', 'a', 'w', 'a', 'y']\n",
      "\n",
      "Original tweet:\n",
      "Dinner in FLA tonight // Pan-seared salmon over couscous veggie salad #yum #dinner #florida #salmon…\n",
      "Word tokenization:\n",
      "['dinner', 'in', '<unk>', 'tonight', '/', '/', 'pan', '-', 'seared', 'salmon', 'over', '<unk>', 'veggie', 'salad', '#', 'yum', '#', 'dinner', '#', '<unk>', '#', 'salmon', '…']\n",
      "BPE tokenization:\n",
      "['d', 'in', 'n', 'er__', 'in__', 'f', 'l', 'a__', 't', 'on', 'i', 'gh', 't__', '/', '__', '/', '__', 'p', 'an__', '-', '__', 'se', 'ar', 'e', 'd__', 's', 'al', 'm', 'on__', 'o', 'v', 'er__', 'c', 'ou', 's', 'c', 'ou', 's__', 've', 'g', 'g', 'i', 'e__', 's', 'al', 'a', 'd__', '#__', 'y', 'u', 'm', '__', '#__', 'd', 'in', 'n', 'er__', '#__', 'f', 'l', 'or', 'i', 'd', 'a__', '#__', 's', 'al', 'm', 'on__', '…']\n",
      "\n",
      "Original tweet:\n",
      "It's my fav seniors last game congrats on beating west @ West Salem…\n",
      "Word tokenization:\n",
      "['it', 'is', 'my', 'fav', 'seniors', 'last', 'game', 'congrats', 'on', 'beating', 'west', '@', 'west', 'salem', '…']\n",
      "BPE tokenization:\n",
      "['it__', 'is__', 'my__', 'f', 'a', 'v', '__', 's', 'en', 'i', 'or', 's__', 'la', 's', 't__', 'g', 'am', 'e__', 'c', 'on', 'g', 'r', 'at', 's__', 'on__', 'be', 'at', 'ing__', 'we', 's', 't__', '@__', 'we', 's', 't__', 's', 'al', 'e', 'm', '__', '…']\n",
      "\n",
      "Original tweet:\n",
      "I got to to go formal with my best friend @ Phi Mu at JSU\n",
      "Word tokenization:\n",
      "['i', 'got', 'to', 'to', 'go', 'formal', 'with', 'my', 'best', 'friend', '@', 'phi', 'mu', 'at', '<unk>']\n",
      "BPE tokenization:\n",
      "['i__', 'g', 'o', 't__', 'to__', 'to__', 'g', 'o__', 'for', 'm', 'al', '__', 'with__', 'my__', 'be', 's', 't__', 'f', 'ri', 'en', 'd__', '@__', 'p', 'h', 'i__', 'm', 'u', '__', 'a', 't__', 'j', 's', 'u']\n",
      "\n",
      "Original tweet:\n",
      "'Cause I Miss My Little Homies .#Throwback #CousinLove @ Indiana University\n",
      "Word tokenization:\n",
      "[\"'\", 'cause', 'i', 'miss', 'my', 'little', 'homies', '.', '#', 'throwback', '#', 'cousinlove', '@', '<unk>', 'university']\n",
      "BPE tokenization:\n",
      "[\"'\", '__', 'c', 'a', 'u', 's', 'e__', 'i__', 'm', 'i', 's', 's__', 'my__', 'li', 't', 't', 'l', 'e__', 'h', 'om', 'i', 'es__', '.__', '#__', 'th', 'ro', 'w', 'b', 'ac', 'k__', '#__', 'c', 'ou', 's', 'in', 'love__', '@__', 'in', 'di', 'an', 'a__', 'u', 'ni', 'v', 'er', 'si', 't', 'y']\n",
      "\n",
      "Original tweet:\n",
      "Birthday Kisses @ Madison, Wisconsin\n",
      "Word tokenization:\n",
      "['birthday', 'kisses', '@', 'madison', ',', '<unk>']\n",
      "BPE tokenization:\n",
      "['b', 'ir', 'th', 'day__', 'k', 'i', 's', 's', 'es__', '@__', 'ma', 'di', 's', 'on__', ',__', 'wi', 's', 'c', 'on', 's', 'in']\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 10: Comparing tokenizers\n",
    "\n",
    "Train the BPE tokenizer with different number of merges. Compare the tokenization results with the word tokenization.\n",
    "1. (5p) What are the differences?\n",
    "2. (5p) Compare the number of tokens created by your tokenizers.\n",
    "3. (5p) Calculate the number of `<unk>` tokens in the validation dataset for each tokenizer.\n",
    "4. (5p) Compare the average length in tokens between different tokenizers.\n",
    "5. (5p) What are the advantages and disadvantages of the BPE tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
